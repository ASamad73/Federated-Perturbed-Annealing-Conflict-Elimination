{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a7b40f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:31:18.624886Z",
     "iopub.status.busy": "2025-12-26T06:31:18.624232Z",
     "iopub.status.idle": "2025-12-26T06:31:30.860586Z",
     "shell.execute_reply": "2025-12-26T06:31:30.859849Z"
    },
    "id": "rXh2D0ZW3ywP",
    "outputId": "707e062d-b5d0-47da-d796-5730f03cefef",
    "papermill": {
     "duration": 12.243148,
     "end_time": "2025-12-26T06:31:30.862227",
     "exception": false,
     "start_time": "2025-12-26T06:31:18.619079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 0) Environment setup (run on Colab)\n",
    "# If you start a fresh Colab runtime you can uncomment below to install torch if necessary:\n",
    "# !pip install torch torchvision --quiet\n",
    "\n",
    "import torch, torchvision, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np, random, time, os\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, resnet18\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "\n",
    "seed = 0\\\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "# 0) Environment setup (run on Colab)\n",
    "# If you start a fresh Colab runtime you can uncomment below to install torch if necessary:\n",
    "# !pip install torch torchvision --quiet\n",
    "# --- Standard Library ---\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# --- Third-Party Data & Plotting ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- PyTorch Core ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# --- PyTorch Data Utilities ---\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "\n",
    "# --- Torchvision ---\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.models import resnet18, resnet50\n",
    "\n",
    "# Note: 'CIFAR10' is available via 'datasets.CIFAR10'\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "seed = 0\\\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b8b348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:31:30.869841Z",
     "iopub.status.busy": "2025-12-26T06:31:30.869131Z",
     "iopub.status.idle": "2025-12-26T06:31:30.876068Z",
     "shell.execute_reply": "2025-12-26T06:31:30.875496Z"
    },
    "id": "PLZbPrR_3_kp",
    "papermill": {
     "duration": 0.011888,
     "end_time": "2025-12-26T06:31:30.877296",
     "exception": false,
     "start_time": "2025-12-26T06:31:30.865408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Utilities: flattening, cosine similarity, pairwise stats\n",
    "def flat_params_from_model(model):\n",
    "  return parameters_to_vector([p for p in model.parameters() if p.requires_grad]).detach().cpu()\n",
    "\n",
    "def set_flat_params_to_model(model, flat_vec):\n",
    "  if isinstance(flat_vec, (list, np.ndarray)):\n",
    "    flat_vec = torch.from_numpy(np.array(flat_vec))\n",
    "  vector_to_parameters(flat_vec.to(next(model.parameters()).device), [p for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "def cosine_sim_np(a, b):\n",
    "  a = np.asarray(a, dtype=float).ravel()\n",
    "  b = np.asarray(b, dtype=float).ravel()\n",
    "  denom = (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12)\n",
    "  return float(np.dot(a, b) / denom)\n",
    "\n",
    "def pairwise_cosine_stats(list_of_flat_grads):\n",
    "  n = len(list_of_flat_grads)\n",
    "  sims = []\n",
    "  for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "      sims.append(cosine_sim_np(list_of_flat_grads[i], list_of_flat_grads[j]))\n",
    "  if len(sims) == 0:\n",
    "    return 0.0, 0.0\n",
    "  return float(np.min(sims)), float(np.mean(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c3007d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:31:30.883890Z",
     "iopub.status.busy": "2025-12-26T06:31:30.883631Z",
     "iopub.status.idle": "2025-12-26T06:31:30.888210Z",
     "shell.execute_reply": "2025-12-26T06:31:30.887596Z"
    },
    "id": "l0gYJ5hP4IgP",
    "papermill": {
     "duration": 0.009427,
     "end_time": "2025-12-26T06:31:30.889501",
     "exception": false,
     "start_time": "2025-12-26T06:31:30.880074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) Fed-GGA utilities (seed-based perturbations & client scoring)\n",
    "def sample_k_seeds(K, base_seed=None):\n",
    "  rng = np.random.RandomState(base_seed)\n",
    "  return [int(rng.randint(0, 2**31 - 1)) for _ in range(K)]\n",
    "\n",
    "def get_heldout_split(domain_loaders, held_out):\n",
    "  assert held_out in domain_loaders, f\"{held_out} is not a valid domain!\"\n",
    "\n",
    "  test_loader = domain_loaders[held_out]\n",
    "  train_loaders = [dl for name, dl in domain_loaders.items() if name != held_out]\n",
    "\n",
    "  return train_loaders, test_loader, held_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c383f444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:31:30.896243Z",
     "iopub.status.busy": "2025-12-26T06:31:30.896014Z",
     "iopub.status.idle": "2025-12-26T06:31:30.906220Z",
     "shell.execute_reply": "2025-12-26T06:31:30.905685Z"
    },
    "papermill": {
     "duration": 0.015202,
     "end_time": "2025-12-26T06:31:30.907442",
     "exception": false,
     "start_time": "2025-12-26T06:31:30.892240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "\n",
    "GradientDict = Dict[str, torch.Tensor]\n",
    "\n",
    "class AggregationStrategy(ABC):\n",
    "    \"\"\"Abstract base class for gradient aggregation strategies.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def aggregate(self, gradients: List[GradientDict]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Aggregate gradients from multiple clients.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class AgreementWeightedFedAvg(AggregationStrategy):\n",
    "    \"\"\"\n",
    "    Agreement-Weighted Federated Averaging.\n",
    "    \n",
    "    Computes per-parameter agreement weights based on sign consensus:\n",
    "    W_j = |Σ sign((g_i)_j)| / N\n",
    "    \n",
    "    Final update: g_avg ⊙ W (element-wise multiplication)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = False):\n",
    "        self.verbose = verbose\n",
    "        self.last_agreement_weights = {}\n",
    "    \n",
    "    def compute_average_gradient(\n",
    "        self, \n",
    "        gradients: List[GradientDict]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Compute g_avg = (1/N) * Σ g_i\"\"\"\n",
    "        n = len(gradients)\n",
    "        averaged = {}\n",
    "        \n",
    "        param_names = gradients[0].gradients.keys()\n",
    "        \n",
    "        for name in param_names:\n",
    "            grad_sum = sum(g.gradients[name] for g in gradients)\n",
    "            averaged[name] = grad_sum / n\n",
    "        \n",
    "        return averaged\n",
    "    \n",
    "    def compute_agreement_weights(\n",
    "        self, \n",
    "        gradients: List[GradientDict]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute per-parameter agreement weights:\n",
    "        W_j = |Σ sign((g_i)_j)| / N\n",
    "        \"\"\"\n",
    "        n = len(gradients)\n",
    "        weights = {}\n",
    "        \n",
    "        param_names = gradients[0].gradients.keys()\n",
    "        \n",
    "        for name in param_names:\n",
    "            # Sum of signs for each parameter across all clients\n",
    "            sign_sum = sum(torch.sign(g.gradients[name]) for g in gradients)\n",
    "            \n",
    "            # Absolute value normalized by number of clients\n",
    "            weights[name] = torch.abs(sign_sum) / n\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def aggregate(self, gradients: List[GradientDict]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Aggregate gradients using agreement weighting.\n",
    "        \n",
    "        Returns: g_avg ⊙ W\n",
    "        \"\"\"\n",
    "        if not gradients:\n",
    "            raise ValueError(\"Cannot aggregate empty gradient list\")\n",
    "        \n",
    "        # Step 1: Compute average gradient\n",
    "        g_avg = self.compute_average_gradient(gradients)\n",
    "        \n",
    "        # Step 2: Compute agreement weights\n",
    "        weights = self.compute_agreement_weights(gradients)\n",
    "        self.last_agreement_weights = weights\n",
    "        \n",
    "        # Step 3: Apply element-wise multiplication\n",
    "        weighted_gradient = {\n",
    "            name: g_avg[name] * weights[name]\n",
    "            for name in g_avg.keys()\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            for name in weights.keys():\n",
    "                w = weights[name]\n",
    "                print(f\"  {name}: Agreement - Min: {w.min():.3f}, \"\n",
    "                      f\"Max: {w.max():.3f}, Mean: {w.mean():.3f}\")\n",
    "        \n",
    "        return weighted_gradient\n",
    "    \n",
    "    def get_agreement_statistics(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get statistics about the last computed agreement weights.\"\"\"\n",
    "        stats = {}\n",
    "        for name, weight in self.last_agreement_weights.items():\n",
    "            stats[name] = {\n",
    "                \"min\": float(weight.min()),\n",
    "                \"max\": float(weight.max()),\n",
    "                \"mean\": float(weight.mean()),\n",
    "                \"std\": float(weight.std())\n",
    "            }\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c945a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:31:30.914049Z",
     "iopub.status.busy": "2025-12-26T06:31:30.913823Z",
     "iopub.status.idle": "2025-12-26T06:31:30.923475Z",
     "shell.execute_reply": "2025-12-26T06:31:30.922916Z"
    },
    "papermill": {
     "duration": 0.014576,
     "end_time": "2025-12-26T06:31:30.924719",
     "exception": false,
     "start_time": "2025-12-26T06:31:30.910143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PruningFedAvg(AggregationStrategy):\n",
    "    def __init__(self, threshold: float = 0.3, patience: int = 5):\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.pruning_rate = 0.0\n",
    "        # Memory to store how many consecutive rounds a parameter has been in conflict\n",
    "        self.consecutive_conflict_counts = {} \n",
    "        self.last_agreement_weights = {}\n",
    "\n",
    "    def compute_agreement_weights(self, gradients: List[GradientDict]):\n",
    "        n_clients = len(gradients)\n",
    "\n",
    "        weights = {}\n",
    "        for gradient in gradients:\n",
    "            for name, grad in gradient.gradients.items():\n",
    "                if name not in weights:\n",
    "                    weights[name] = torch.zeros_like(grad)\n",
    "                weights[name] += torch.sign(grad)\n",
    "        for name in weights:\n",
    "            weights[name] = torch.abs(weights[name]) / n_clients\n",
    "        return weights\n",
    "\n",
    "    def aggregate(self, gradients: List[GradientDict]):\n",
    "        N = len(gradients)\n",
    "        param_names = gradients[0].gradients.keys()\n",
    "        \n",
    "        # 1. Calculate standard average gradient\n",
    "        g_avg = {name: sum(g.gradients[name] for g in gradients) / N for name in param_names}\n",
    "        \n",
    "\n",
    "        weights = self.compute_agreement_weights(gradients)\n",
    "\n",
    "        self.last_agreement_weights = weights\n",
    "        \n",
    "        for name in g_avg.keys():\n",
    "            g_avg[name] = g_avg[name] * weights[name]\n",
    "\n",
    "        pruned_grads = {}\n",
    "        total_params, pruned_params = 0, 0\n",
    "        \n",
    "        for name in param_names:\n",
    "            # 2. Calculate Agreement W_j\n",
    "            sign_sum = sum(torch.sign(g.gradients[name]) for g in gradients)\n",
    "            agreement = torch.abs(sign_sum) / N\n",
    "            \n",
    "            # Initialize counter for this parameter layer if it doesn't exist\n",
    "            if name not in self.consecutive_conflict_counts:\n",
    "                self.consecutive_conflict_counts[name] = torch.zeros_like(agreement)\n",
    "\n",
    "            # 3. Check for conflict: If agreement < threshold, increment count. Else, RESET to 0.\n",
    "            # (agreement < self.threshold) creates a boolean mask\n",
    "            has_conflict = (agreement < self.threshold)\n",
    "            \n",
    "            # Increment where there is conflict\n",
    "            self.consecutive_conflict_counts[name] += has_conflict.float()\n",
    "            \n",
    "            # IMPORTANT: Reset count to 0 for any parameter that NOW reaches consensus\n",
    "            self.consecutive_conflict_counts[name] *= has_conflict.float()\n",
    "\n",
    "            # 4. Generate the Pruning Mask\n",
    "            # Mask is 0 only if the conflict count has reached the 'patience' limit (e.g., 5)\n",
    "            mask = (self.consecutive_conflict_counts[name] < self.patience).float()\n",
    "            \n",
    "            # 5. Apply Pruning\n",
    "            pruned_grads[name] = g_avg[name] * mask\n",
    "            \n",
    "            # Track statistics\n",
    "            total_params += mask.numel()\n",
    "            pruned_params += (mask == 0).sum().item()\n",
    "            \n",
    "        pruning_rate = (pruned_params / total_params) * 100\n",
    "        self.pruning_rate = pruning_rate\n",
    "        return pruned_grads, pruning_rate\n",
    "\n",
    "\n",
    "    def get_agreement_statistics(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get statistics about the last computed agreement weights.\"\"\"\n",
    "        stats = {}\n",
    "        for name, weight in self.last_agreement_weights.items():\n",
    "            stats[name] = {\n",
    "                \"min\": float(weight.min()),\n",
    "                \"max\": float(weight.max()),\n",
    "                \"mean\": float(weight.mean()),\n",
    "                \"std\": float(weight.std())\n",
    "            }\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b237b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:31:30.931269Z",
     "iopub.status.busy": "2025-12-26T06:31:30.931055Z",
     "iopub.status.idle": "2025-12-26T06:32:00.764463Z",
     "shell.execute_reply": "2025-12-26T06:32:00.763576Z"
    },
    "id": "AEOJEMNuC9PE",
    "outputId": "1e9f9e96-d3f0-4f90-ffc2-d3dd84fbe2bb",
    "papermill": {
     "duration": 29.838532,
     "end_time": "2025-12-26T06:32:00.765997",
     "exception": false,
     "start_time": "2025-12-26T06:31:30.927465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddb4741559b47538d4f5a0af7bbfa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26dd4289cfb427f80f6d3ccf5142d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/191M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fff9f27209f4166b57f53fcf00948f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF PACS domains found: ['art_painting', 'cartoon', 'photo', 'sketch']\n",
      "Built domain_loaders: ['art_painting', 'cartoon', 'photo', 'sketch']\n",
      "Sample batch from domain=art_painting: images torch.Size([32, 3, 224, 224]), labels torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"flwrlabs/pacs\")\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "class HFPACSDomainDataset(Dataset):\n",
    "  def __init__(self, hf_ds, domain_name, transform=None):\n",
    "    self.transform = transform\n",
    "    self.indices = [i for i, ex in enumerate(hf_ds) if ex[\"domain\"] == domain_name]\n",
    "    self.hf_ds = hf_ds\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.indices)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    ex = self.hf_ds[self.indices[idx]]\n",
    "    img = ex[\"image\"]           \n",
    "    label = int(ex[\"label\"])    \n",
    "    if self.transform is not None:\n",
    "      img = self.transform(img)\n",
    "    return img, label\n",
    "\n",
    "# Inspect domains present in the HF dataset (sanity)\n",
    "hf_root = ds  # dataset dict returned by load_dataset\n",
    "# If dataset has a single split named \"train\", use that\n",
    "split_name = list(hf_root.keys())[0]   # usually \"train\"\n",
    "hf_split = hf_root[split_name]\n",
    "\n",
    "# Collect unique domains (should be 4)\n",
    "all_domains = sorted(list(set(hf_split[\"domain\"])))\n",
    "print(\"HF PACS domains found:\", all_domains)\n",
    "\n",
    "# Build domain-specific PyTorch DataLoaders (match your previous batch size / shuffle choices)\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "\n",
    "domain_loaders = {}\n",
    "for dom in all_domains:\n",
    "    # For 'sketch' use test transform (as you did) otherwise train_transform\n",
    "    tf = test_transform if dom == \"sketch\" else train_transform\n",
    "    ds_dom = HFPACSDomainDataset(hf_split, dom, transform=tf)\n",
    "    # shuffle=True for training domains, keep sketch shuffle consistent with earlier (you used shuffle=True for sketch earlier too)\n",
    "    shuffle_flag = True if dom != \"cartoon\" else False  # follow your original: cartoon had shuffle=False, adjust if needed\n",
    "    loader = DataLoader(ds_dom, batch_size=batch_size, shuffle=shuffle_flag, num_workers=num_workers)\n",
    "    domain_loaders[dom] = loader\n",
    "\n",
    "# ensure ordering same as before and create train/test split using your helper\n",
    "domain_names = list(domain_loaders.keys())\n",
    "print(\"Built domain_loaders:\", domain_names)\n",
    "\n",
    "# Use your helper to get train_loaders and test_loader (held_out = \"sketch\" as before)\n",
    "train_loaders, test_loader, held_out_name = get_heldout_split(domain_loaders, \"sketch\")\n",
    "\n",
    "# quick sanity: print one batch shape from a domain\n",
    "sample_domain = list(domain_loaders.keys())[0]\n",
    "xb, yb = next(iter(domain_loaders[sample_domain]))\n",
    "print(f\"Sample batch from domain={sample_domain}: images {xb.shape}, labels {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f496bc5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:32:00.775160Z",
     "iopub.status.busy": "2025-12-26T06:32:00.774522Z",
     "iopub.status.idle": "2025-12-26T06:32:00.789415Z",
     "shell.execute_reply": "2025-12-26T06:32:00.788820Z"
    },
    "id": "BHhTMDBLIUkw",
    "papermill": {
     "duration": 0.021067,
     "end_time": "2025-12-26T06:32:00.790695",
     "exception": false,
     "start_time": "2025-12-26T06:32:00.769628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uniform delta sampler (paper's U(-rho, rho) per-parameter, scaled by model norm)\n",
    "def make_uniform_delta_from_seed(seed, prototype_vector, rho, device='cpu', scale_by_norm=False):\n",
    "    # use a local generator to avoid global torch RNG side-effects\n",
    "    gen = torch.Generator(device=device)\n",
    "    gen.manual_seed(int(seed) & 0xffffffff)\n",
    "    flat = prototype_vector.to(device)\n",
    "    uni = (torch.rand(flat.shape, generator=gen, device=device) * 2.0 - 1.0) * rho\n",
    "    if scale_by_norm:\n",
    "        model_norm = torch.norm(flat) + 1e-12\n",
    "        delta = uni * model_norm\n",
    "    else:\n",
    "        delta = uni\n",
    "    return delta\n",
    "\n",
    "def client_compute_scores_for_fedgga(model, loss_fn, data_loader, seeds, rho, device='cpu', \n",
    "                                     scale_by_norm=False, search_head_only=True):\n",
    "    \"\"\"\n",
    "    Compute reference gradient on one small batch, then for each seed:\n",
    "      - apply delta IN-PLACE to model parameters (fast)\n",
    "      - forward/backward to get g_k (torch tensor on device)\n",
    "      - compute cosine similarity on device between g_k and ref_grad\n",
    "      - revert the delta IN-PLACE\n",
    "    Returns:\n",
    "      scores (list of float) length == len(seeds),\n",
    "      ref_grad_numpy (np.array),\n",
    "      loss_ref (float),\n",
    "      losses_k (list of float) length == len(seeds)\n",
    "    \"\"\"\n",
    "    local_model = deepcopy(model).to(device)\n",
    "    local_model.train()\n",
    "\n",
    "    # Get one small batch for scoring and move to device\n",
    "    it = iter(data_loader)\n",
    "    xs, ys = next(it)\n",
    "    xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "    # Optionally micro-batch if you want cheaper scoring\n",
    "    # micro_b = min(8, xs.shape[0])\n",
    "    # xs, ys = xs[:micro_b], ys[:micro_b]\n",
    "\n",
    "    # Select parameters that will be perturbed / measured\n",
    "    # param_list = [p for p in local_model.parameters() if p.requires_grad]\n",
    "    if search_head_only:\n",
    "        param_list, param_names = get_head_param_list_and_names(local_model)\n",
    "    else:\n",
    "        param_list = [p for p in local_model.parameters() if p.requires_grad]\n",
    "    if len(param_list) == 0:\n",
    "        return [0.0] * len(seeds), np.zeros(1), 0.0, [0.0] * len(seeds)\n",
    "        \n",
    "    numels = [p.numel() for p in param_list]\n",
    "    flat_theta = parameters_to_vector(param_list).detach().to(device)\n",
    "\n",
    "    local_model.zero_grad()\n",
    "    out = local_model(xs)\n",
    "    loss_ref_tensor = loss_fn(out, ys)\n",
    "    loss_ref = float(loss_ref_tensor.detach().cpu().item())\n",
    "    loss_ref_tensor.backward()\n",
    "\n",
    "    ref_grad_parts = []\n",
    "    for p in param_list:\n",
    "        g = p.grad\n",
    "        if g is None:\n",
    "            ref_grad_parts.append(torch.zeros(p.numel(), device=device))\n",
    "        else:\n",
    "            ref_grad_parts.append(g.detach().view(-1))\n",
    "    ref_grad_t = torch.cat(ref_grad_parts)            # on device\n",
    "    ref_grad_numpy = ref_grad_t.detach().cpu().numpy()\n",
    "\n",
    "    scores = []\n",
    "    losses_k = []\n",
    "\n",
    "    # Helper: apply delta in-place and revert\n",
    "    def apply_delta_inplace(delta_flat):\n",
    "        offset = 0\n",
    "        for p, n in zip(param_list, numels):\n",
    "            seg = delta_flat[offset: offset + n].view_as(p.data)\n",
    "            p.data.add_(seg)\n",
    "            offset += n\n",
    "\n",
    "    def revert_delta_inplace(delta_flat):\n",
    "        offset = 0\n",
    "        for p, n in zip(param_list, numels):\n",
    "            seg = delta_flat[offset: offset + n].view_as(p.data)\n",
    "            p.data.sub_(seg)\n",
    "            offset += n\n",
    "\n",
    "    # Loop over all candidate seeds -> produce one score per seed\n",
    "    for seed in seeds:\n",
    "        delta = make_uniform_delta_from_seed(seed, flat_theta, rho, device=device, scale_by_norm=scale_by_norm)\n",
    "        apply_delta_inplace(delta)\n",
    "\n",
    "        local_model.zero_grad()\n",
    "        out_k = local_model(xs)\n",
    "        loss_k_tensor = loss_fn(out_k, ys)\n",
    "        loss_k = float(loss_k_tensor.detach().cpu().item())\n",
    "        loss_k_tensor.backward()\n",
    "\n",
    "        # collect gk as a single torch tensor (on device)\n",
    "        gk_parts = []\n",
    "        for p in param_list:\n",
    "            g = p.grad\n",
    "            if g is None:\n",
    "                gk_parts.append(torch.zeros(p.numel(), device=device))\n",
    "            else:\n",
    "                gk_parts.append(g.detach().view(-1))\n",
    "        gk_t = torch.cat(gk_parts)\n",
    "\n",
    "        # cosine sim on device (use small eps)\n",
    "        denom = (torch.norm(gk_t) * torch.norm(ref_grad_t) + 1e-12)\n",
    "        sim_t = float(torch.dot(gk_t, ref_grad_t).item() / denom.item())\n",
    "\n",
    "        scores.append(sim_t)\n",
    "        losses_k.append(loss_k)\n",
    "\n",
    "        # revert delta in-place\n",
    "        revert_delta_inplace(delta)\n",
    "\n",
    "    # Safety: ensure local_model params exactly restored (optional)\n",
    "    set_flat_params_to_model(local_model, flat_theta)\n",
    "\n",
    "    return scores, ref_grad_numpy, loss_ref, losses_k\n",
    "\n",
    "def fedavg_from_state_dicts(state_dicts):\n",
    "    \"\"\"\n",
    "    Given a list of PyTorch state_dict() objects (assumed identical keys),\n",
    "    return a new state_dict that is the simple average of the tensors.\n",
    "    \"\"\"\n",
    "    if len(state_dicts) == 0:\n",
    "        raise ValueError(\"No state dicts provided to fedavg_from_state_dicts\")\n",
    "    n = len(state_dicts)\n",
    "    keys = list(state_dicts[0].keys())\n",
    "    new_sd = {}\n",
    "    for k in keys:\n",
    "        # sum up as float32 to avoid dtype issues\n",
    "        accum = None\n",
    "        for sd in state_dicts:\n",
    "            v = sd[k].cpu().float()\n",
    "            if accum is None:\n",
    "                accum = v.clone()\n",
    "            else:\n",
    "                accum += v\n",
    "        new_sd[k] = (accum / float(n))\n",
    "    return new_sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87784fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:32:00.798491Z",
     "iopub.status.busy": "2025-12-26T06:32:00.798286Z",
     "iopub.status.idle": "2025-12-26T06:32:00.840889Z",
     "shell.execute_reply": "2025-12-26T06:32:00.840188Z"
    },
    "id": "oTFnv8S4A4To",
    "papermill": {
     "duration": 0.048443,
     "end_time": "2025-12-26T06:32:00.842329",
     "exception": false,
     "start_time": "2025-12-26T06:32:00.793886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import mode\n",
    "\n",
    "\n",
    "class FedClient:\n",
    "    def __init__(self, name, train_loader, test_loader, device):\n",
    "        self.name = name\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "\n",
    "    def score_seeds(self, model, loss_fn, seeds, rho, scale_by_norm, search_head_only):\n",
    "        return client_compute_scores_for_fedgga(model, loss_fn, self.train_loader, seeds, rho, device=self.device, \n",
    "                                                scale_by_norm=scale_by_norm, search_head_only=search_head_only)\n",
    "\n",
    "    def local_update(self, global_model, local_epochs=1, lr=0.01, max_steps=None, use_amp=False):\n",
    "        model = deepcopy(global_model).to(self.device)\n",
    "        # opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "        if len(trainable_params) == 0:\n",
    "            return deepcopy(model.state_dict())\n",
    "            \n",
    "        opt = torch.optim.Adam(trainable_params, lr=lr)\n",
    "            \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        model.train()\n",
    "\n",
    "        scaler = GradScaler() if (use_amp and self.device.startswith('cuda')) else None\n",
    "\n",
    "        step = 0\n",
    "        for _ in range(local_epochs):\n",
    "            for xb, yb in self.train_loader:\n",
    "                xb, yb = xb.to( self.device), yb.to( self.device)\n",
    "                opt.zero_grad()\n",
    "                if scaler is not None:\n",
    "                    with autocast():\n",
    "                        logits = model(xb)\n",
    "                        loss = loss_fn(logits, yb)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    logits = model(xb)\n",
    "                    loss = loss_fn(logits, yb)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                step += 1\n",
    "                if (max_steps is not None) and (step >= max_steps):\n",
    "                    break\n",
    "            if (max_steps is not None) and (step >= max_steps):\n",
    "                break\n",
    "\n",
    "        return deepcopy(model.state_dict())\n",
    "    \n",
    "    def compute_avg_gradient(self, global_model, local_epochs=1, max_batches=None, device=None):\n",
    "        \"\"\"\n",
    "        Compute average gradients over local data WITHOUT applying optimizer steps.\n",
    "        Use a deepcopy of `global_model` (server model) so we don't require client-local model attr.\n",
    "        Returns: dict mapping parameter name -> gradient tensor (on CPU).\n",
    "        \"\"\"\n",
    "        device = device or self.device\n",
    "        # local copy of server/global model to avoid modifying server state\n",
    "        model = deepcopy(global_model).to(device)\n",
    "        model.train()\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        accumulated = None\n",
    "        batch_count = 0\n",
    "        step = 0\n",
    "\n",
    "        for ep in range(local_epochs):\n",
    "            for xb, yb in self.train_loader:\n",
    "                if (max_batches is not None) and (step >= max_batches):\n",
    "                    break\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                model.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "\n",
    "                # collect this batch grads (move to CPU to keep memory predictable)\n",
    "                batch_grads = {}\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        batch_grads[name] = param.grad.detach().cpu().clone()\n",
    "\n",
    "                if accumulated is None:\n",
    "                    accumulated = {k: v.clone() for k, v in batch_grads.items()}\n",
    "                else:\n",
    "                    for k, v in batch_grads.items():\n",
    "                        if k in accumulated:\n",
    "                            accumulated[k] += v\n",
    "                        else:\n",
    "                            accumulated[k] = v.clone()\n",
    "\n",
    "                batch_count += 1\n",
    "                step += 1\n",
    "            if (max_batches is not None) and (step >= max_batches):\n",
    "                break\n",
    "\n",
    "        if accumulated is None:\n",
    "            return {}\n",
    "\n",
    "        for k in accumulated:\n",
    "            accumulated[k] = accumulated[k] / float(batch_count)\n",
    "\n",
    "        return accumulated  \n",
    "\n",
    "\n",
    "    def eval_on_test(self, model):\n",
    "        model = deepcopy(model).to(self.device)\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in self.test_loader:\n",
    "                x,y = x.to(self.device), y.to(self.device)\n",
    "                preds = model(x).argmax(dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        return correct / total if total > 0 else 0.0\n",
    "\n",
    "class FedGGAServer:\n",
    "    def __init__(self, server_model, clients, device, config, test_loader):\n",
    "        self.model = deepcopy(server_model).to(device)\n",
    "        self.clients = clients\n",
    "        self.device = device\n",
    "        self.config = config.copy()\n",
    "        self.log = []\n",
    "        self.W_history = []\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def run(self):\n",
    "        cfg = self.config\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        extra_grad_evals = 0\n",
    "        for rnd in range(cfg['rounds']):\n",
    "            t0 = time.time()\n",
    "            if cfg['R_start'] <= rnd <= cfg['R_end'] and cfg['enable_gga']:\n",
    "                seeds = sample_k_seeds(cfg['K'], base_seed=cfg.get('base_seed', 1234) + rnd)\n",
    "                client_scores = []   # shape (n_clients, K)\n",
    "                client_ref_grads = []\n",
    "                client_ref_losses = []\n",
    "                client_losses_k = [] # list of lists: for each client, list of K losses\n",
    "\n",
    "                # print(\"clients starting\")\n",
    "                # Each client computes scores and returns ref_grad & losses\n",
    "                for c in self.clients:\n",
    "                    scores, ref_grad, ref_loss, losses_k = c.score_seeds(self.model, loss_fn, seeds, cfg['rho'], \n",
    "                                                             scale_by_norm=cfg.get('scale_by_norm', False),\n",
    "                                                             search_head_only=cfg.get('search_last_layer_only', True))\n",
    "                    client_scores.append(scores)\n",
    "                    client_ref_grads.append(ref_grad)\n",
    "                    client_ref_losses.append(ref_loss)\n",
    "                    client_losses_k.append(losses_k)\n",
    "                    extra_grad_evals += (1 + len(seeds)) * 1  # approx: 1 ref + K candidates per client\n",
    "\n",
    "                # compute reference sim LB: min pairwise among client_ref_grads\n",
    "                sim_ref_min, sim_ref_mean = pairwise_cosine_stats(client_ref_grads)\n",
    "                LB = float(np.mean(client_ref_losses))  # average ref loss across clients\n",
    "\n",
    "                # aggregate candidate scores/losses across clients\n",
    "                arr_scores = np.stack(client_scores, axis=0)  # (n_clients, K)\n",
    "                avg_scores = np.mean(arr_scores, axis=0)     # average of per-client sim proxies\n",
    "                arr_losses_k = np.stack(client_losses_k, axis=0)  # (n_clients, K)\n",
    "                avg_losses_k = np.mean(arr_losses_k, axis=0)\n",
    "\n",
    "                # apply acceptance rule per candidate: avg_scores[k] > sim_ref_min AND avg_losses_k[k] - LB < loss_relax\n",
    "                loss_relax = cfg.get('loss_relax', 0.1)\n",
    "                accepted_indices = []\n",
    "                for k_idx in range(len(seeds)):\n",
    "                    if (avg_scores[k_idx] > sim_ref_min) and ((avg_losses_k[k_idx] - LB) < loss_relax):\n",
    "                        accepted_indices.append(k_idx)\n",
    "\n",
    "                # choose best among accepted by highest avg_score (if none accepted, choose best avg_score but only if not too much loss)\n",
    "                if len(accepted_indices) > 0:\n",
    "                    best_k = int(np.argmax(avg_scores[accepted_indices]))\n",
    "                    best_k = accepted_indices[best_k]\n",
    "                else:\n",
    "                    # fallback: choose argmax avg_scores but require loss condition (if not satisfied, skip applying any delta)\n",
    "                    best_k = int(np.argmax(avg_scores))\n",
    "                    if not ((avg_losses_k[best_k] - LB) < loss_relax):\n",
    "                        best_k = None\n",
    "\n",
    "                # apply delta if best_k exists\n",
    "                if best_k is not None:\n",
    "                    best_seed = seeds[best_k]\n",
    "                    if cfg.get('search_last_layer_only', True):\n",
    "                        server_param_list, server_param_names = get_head_param_list_and_names(self.model)\n",
    "                    else:\n",
    "                        server_param_list = [p for p in self.model.parameters() if p.requires_grad]\n",
    "                    \n",
    "                    flat_theta = parameters_to_vector(server_param_list).detach().to(self.device)\n",
    "                    delta = make_uniform_delta_from_seed(best_seed, flat_theta.cpu(), cfg['rho'], device=self.device,\n",
    "                                                        scale_by_norm=cfg.get('scale_by_norm'))\n",
    "                    beta = cfg.get('beta', 1.0)\n",
    "                    new_flat = (flat_theta.to(self.device) + beta * delta.to(self.device)).clone()\n",
    "                    \n",
    "                    offset = 0\n",
    "                    for p, n in zip(server_param_list, [p.numel() for p in server_param_list]):\n",
    "                        seg = new_flat[offset: offset + n].view_as(p.data)\n",
    "                        p.data.copy_(seg)\n",
    "                        offset += n\n",
    "                    # set_flat_params_to_model(self.model, new_flat)\n",
    "                    applied = True\n",
    "                else:\n",
    "                    applied = False\n",
    "\n",
    "                # diagnostics\n",
    "                min_sim = sim_ref_min\n",
    "                mean_sim = sim_ref_mean\n",
    "\n",
    "            else:\n",
    "                min_sim, mean_sim, applied = None, None, False\n",
    "\n",
    "            client_state_dicts = []\n",
    "            client_flat_updates = []\n",
    "\n",
    "            if cfg['enable_dampening'] and (rnd >= cfg.get(\"D_start\") and rnd < cfg.get(\"P_start\")):\n",
    "                aggregation = AgreementWeightedFedAvg(verbose=cfg.get('agg_verbose', False))\n",
    "            \n",
    "                # Use server model param order (only trainable params) to ensure consistent flattening\n",
    "                server_param_names = [name for name, p in self.model.named_parameters() if p.requires_grad]\n",
    "                param_numels = {name: int(p.numel()) for name, p in self.model.named_parameters() if p.requires_grad}\n",
    "            \n",
    "                # 1) compute avg gradients per client (CPU tensors) using the current server model snapshot\n",
    "                gradient_dicts = []\n",
    "                for c in self.clients:\n",
    "                    grads_cpu = c.compute_avg_gradient(\n",
    "                        global_model=self.model,\n",
    "                        local_epochs=cfg['local_epochs'],\n",
    "                        max_batches=cfg['max_client_steps'],\n",
    "                        device=self.device\n",
    "                    )\n",
    "                    if not grads_cpu:\n",
    "                        # client didn't return gradients (e.g. empty loader) -> skip\n",
    "                        continue\n",
    "            \n",
    "                    # normalize/ensure grads_cpu has CPU torch.Tensor values for server_param_names\n",
    "                    # fill missing params with zeros of correct size\n",
    "                    grads_fixed = {}\n",
    "                    for name in server_param_names:\n",
    "                        if name in grads_cpu:\n",
    "                            # ensure it's a torch.Tensor on CPU\n",
    "                            t = grads_cpu[name]\n",
    "                            if isinstance(t, np.ndarray):\n",
    "                                t = torch.from_numpy(t)\n",
    "                            grads_fixed[name] = t.detach().cpu().clone()\n",
    "                        else:\n",
    "                            grads_fixed[name] = torch.zeros(param_numels[name], dtype=torch.float32)\n",
    "            \n",
    "                    # minimal wrapper object expected by AgreementWeightedFedAvg (has .gradients and .client_id)\n",
    "                    class _G:\n",
    "                        def __init__(self, d, cid):\n",
    "                            self.gradients = d\n",
    "                            self.client_id = cid\n",
    "                    gradient_dicts.append(_G(grads_fixed, c.name))\n",
    "            \n",
    "                # If no gradients, fallback to normal FedAvg local updates (keeps loop central)\n",
    "                if len(gradient_dicts) == 0:\n",
    "                    client_state_dicts = []\n",
    "                    for c in self.clients:\n",
    "                        sd = c.local_update(self.model, local_epochs=cfg['local_epochs'],\n",
    "                                            lr=cfg['local_lr'], max_steps=cfg['max_client_steps'], use_amp=cfg['use_amp'])\n",
    "                        client_state_dicts.append(sd)\n",
    "                    new_sd = fedavg_from_state_dicts(client_state_dicts)\n",
    "                    self.model.load_state_dict(new_sd)\n",
    "            \n",
    "                    pct_pruned = 0.0\n",
    "                    damp_W_mean = None\n",
    "                    min_sim, mean_sim = None, None\n",
    "                else:\n",
    "                    # 2) compute flattened numpy vectors (CPU) for pairwise similarity stats\n",
    "                    flat_list = []\n",
    "                    for g in gradient_dicts:\n",
    "                        parts = []\n",
    "                        for name in server_param_names:\n",
    "                            t = g.gradients.get(name)\n",
    "                            if t is None:\n",
    "                                parts.append(np.zeros(param_numels[name], dtype=np.float32))\n",
    "                            else:\n",
    "                                parts.append(t.reshape(-1).numpy())\n",
    "                        flat_vec = np.concatenate(parts).astype(np.float32)\n",
    "                        flat_list.append(flat_vec)\n",
    "            \n",
    "                    # compute pairwise stats (min and mean) using your pairwise_cosine_stats utility\n",
    "                    min_sim, mean_sim = pairwise_cosine_stats(flat_list)  # expects list of 1D numpy arrays\n",
    "            \n",
    "                    # 3) aggregate with AgreementWeightedFedAvg (works on CPU tensors)\n",
    "                    weighted_grad = aggregation.aggregate(gradient_dicts)   # dict: param_name -> CPU tensor\n",
    "            \n",
    "                    # 4) apply aggregated gradient to server model using server_lr_damp (single server update)\n",
    "                    server_lr_damp = cfg.get('server_lr') \n",
    "                    with torch.no_grad():\n",
    "                        for name, param in self.model.named_parameters():\n",
    "                            if name in weighted_grad:\n",
    "                                g_cpu = weighted_grad[name]  # CPU tensor\n",
    "                                # if g_cpu is numpy, convert\n",
    "                                if isinstance(g_cpu, np.ndarray):\n",
    "                                    g_cpu = torch.from_numpy(g_cpu)\n",
    "                                param.data.add_(-server_lr_damp * g_cpu.to(param.device))\n",
    "            \n",
    "                    # diagnostics: agreement stats (optional)\n",
    "                    try:\n",
    "                        damp_W_mean = float(np.mean([float(v.mean()) for v in aggregation.last_agreement_weights.values()])) \\\n",
    "                                     if hasattr(aggregation, 'last_agreement_weights') and aggregation.last_agreement_weights else None\n",
    "                    except Exception:\n",
    "                        damp_W_mean = None\n",
    "            \n",
    "                    pct_pruned = 0.0\n",
    "            \n",
    "            if cfg['enable_pruning'] and (rnd >= cfg.get(\"P_start\") and rnd <= cfg.get(\"rounds\")):\n",
    "                aggregation = PruningFedAvg(threshold=cfg.get('P_tolerance'), patience=cfg.get('P_patience'))\n",
    "            \n",
    "                # 1) collect avg gradients (CPU tensors) from each client using same centralized loop\n",
    "                gradient_dicts = []\n",
    "                param_order = None\n",
    "                for c in self.clients:\n",
    "                    grads_cpu = c.compute_avg_gradient(\n",
    "                        global_model=self.model,\n",
    "                        local_epochs=cfg.get('local_epochs', 1),\n",
    "                        max_batches=cfg.get('max_client_steps', None),\n",
    "                        device=self.device\n",
    "                    )\n",
    "                    if not grads_cpu:\n",
    "                        continue\n",
    "                    if param_order is None:\n",
    "                        param_order = list(grads_cpu.keys())\n",
    "                    class _G:\n",
    "                        def __init__(self, d, cid):\n",
    "                            self.gradients = d\n",
    "                            self.client_id = cid\n",
    "                    gradient_dicts.append(_G(grads_cpu, c.name))\n",
    "            \n",
    "                # If no gradients were collected, fallback to normal FedAvg round so loop remains centralized\n",
    "                if len(gradient_dicts) == 0:\n",
    "                    client_state_dicts = []\n",
    "                    for c in self.clients:\n",
    "                        sd = c.local_update(self.model, local_epochs=cfg['local_epochs'],\n",
    "                                            lr=cfg['local_lr'], max_steps=cfg['max_client_steps'], use_amp=cfg['use_amp'])\n",
    "                        client_state_dicts.append(sd)\n",
    "                    new_sd = fedavg_from_state_dicts(client_state_dicts)\n",
    "                    self.model.load_state_dict(new_sd)\n",
    "                    pct_pruned = 0.0\n",
    "                    damp_W_mean = None\n",
    "                    min_sim, mean_sim = None, None\n",
    "                else:\n",
    "                    # 2) Flatten per-client average gradients (numpy) to compute pairwise similarity diagnostics\n",
    "                    flat_list = []\n",
    "                    for g in gradient_dicts:\n",
    "                        parts = []\n",
    "                        for name in param_order:\n",
    "                            arr = g.gradients.get(name)\n",
    "                            if arr is None:\n",
    "                                parts.append(np.zeros(0, dtype=float))   # unlikely, fallback\n",
    "                            else:\n",
    "                                parts.append(arr.reshape(-1).numpy())\n",
    "                        flat_list.append(np.concatenate(parts))\n",
    "                    # compute pairwise stats (returns min_sim, mean_sim over clients)\n",
    "                    min_sim, mean_sim = pairwise_cosine_stats(flat_list)\n",
    "            \n",
    "                    # 3) Aggregate with PruningFedAvg -> it returns (pruned_grads, pruning_rate)\n",
    "                    pruned_grads, pruning_rate = aggregation.aggregate(gradient_dicts)\n",
    "            \n",
    "                    # 4) Apply pruned gradients to server model in-place using server_lr_prune\n",
    "                    server_lr_prune = cfg.get('server_lr', 1e-3) \n",
    "                    with torch.no_grad():\n",
    "                        for name, param in self.model.named_parameters():\n",
    "                            if name in pruned_grads:\n",
    "                                g_cpu = pruned_grads[name]   # CPU tensor\n",
    "                                param.data.add_(-server_lr_prune * g_cpu.to(self.device))\n",
    "            \n",
    "                    # diagnostics\n",
    "                    pct_pruned = pruning_rate\n",
    "                    try:\n",
    "                        agree_stats = aggregation.get_agreement_statistics()\n",
    "                        damp_W_mean = np.mean([s['mean'] for s in agree_stats.values()]) if len(agree_stats) > 0 else None\n",
    "                    except Exception:\n",
    "                        damp_W_mean = None\n",
    "            \n",
    "            else:\n",
    "                for c in self.clients:\n",
    "                    # print('local update client: ', c.name)\n",
    "                    sd = c.local_update(self.model, local_epochs=cfg['local_epochs'], lr=cfg['local_lr'], max_steps=cfg['max_client_steps'], use_amp=cfg['use_amp'])\n",
    "                    client_state_dicts.append(sd)\n",
    "                    # compute flat update for dampening/pruning\n",
    "                    client_model = deepcopy(self.model)\n",
    "                    client_model.load_state_dict(sd)\n",
    "                    flat_client = parameters_to_vector([p for p in client_model.parameters() if p.requires_grad]).detach().cpu().numpy()\n",
    "                    flat_server = parameters_to_vector([p for p in self.model.parameters() if p.requires_grad]).detach().cpu().numpy()\n",
    "                    client_flat_updates.append(flat_client - flat_server)\n",
    "                \n",
    "                new_sd = fedavg_from_state_dicts(client_state_dicts)\n",
    "                # convert types back to server model's dtype/device if necessary\n",
    "                # load into server model\n",
    "                self.model.load_state_dict(new_sd)\n",
    "                \n",
    "                pct_pruned = 0.0\n",
    "                damp_W_mean = None\n",
    "\n",
    "            # # optionally apply GGA-L instead of full search in non-anneal rounds (cheap)\n",
    "            # if cfg.get('use_gga_l', False) and not (cfg['R_start'] <= rnd <= cfg['R_end'] and cfg['enable_gga']):\n",
    "            #     # estimate mean_sim (use last collected client_ref_grads if exist)\n",
    "            #     if 'client_ref_grads' in locals() and len(client_ref_grads)>0:\n",
    "            #         _, mean_sim_est = pairwise_cosine_stats(client_ref_grads)\n",
    "            #     else:\n",
    "            #         mean_sim_est = 0.0\n",
    "            #     alpha = compute_alpha_from_pairwise_mean(mean_sim_est, gamma=cfg.get('gga_l_gamma', 1e-3))\n",
    "            #     noise = gga_l_noise_vector(self.model, device=self.device)\n",
    "            #     flat = parameters_to_vector([p for p in self.model.parameters() if p.requires_grad]).detach()\n",
    "            #     new_flat = (flat + alpha * noise).clone()\n",
    "            #     set_flat_params_to_model(self.model, new_flat)\n",
    "\n",
    "            # evaluate (clients' local test loaders might be used; for held-out evaluation evaluate separately)\n",
    "            accs = [c.eval_on_test(self.model) for c in self.clients]\n",
    "            avg_acc = float(np.mean(accs))\n",
    "\n",
    "            # logging\n",
    "            self.log.append({\n",
    "                'round': rnd,\n",
    "                'avg_client_acc': avg_acc,\n",
    "                'min_pairwise_sim': min_sim,\n",
    "                'mean_pairwise_sim': mean_sim,\n",
    "                'applied_delta': bool(applied),\n",
    "                'applied_delta': applied,\n",
    "                'pct_pruned': pct_pruned,\n",
    "                'damp_W_mean': damp_W_mean,\n",
    "                'time': time.time() - t0,\n",
    "                'extra_grad_evals_est': extra_grad_evals\n",
    "            })\n",
    "\n",
    "            if rnd%5==0:\n",
    "              print(f\"[R{rnd}] avg_acc={avg_acc:.4f} min_sim={min_sim} mean_sim={mean_sim} applied_delta={applied}\")\n",
    "              # print(f\"[R{rnd}] avg_acc={avg_acc:.4f} min_sim={min_sim} mean_sim={mean_sim} applied_delta={applied} pct_pruned={pct_pruned:.3f}\")\n",
    "\n",
    "        return self.log\n",
    "\n",
    "# ==== END of Fed-GGA implementation ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bbedabb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:32:00.849975Z",
     "iopub.status.busy": "2025-12-26T06:32:00.849749Z",
     "iopub.status.idle": "2025-12-26T06:32:00.855884Z",
     "shell.execute_reply": "2025-12-26T06:32:00.855245Z"
    },
    "papermill": {
     "duration": 0.011614,
     "end_time": "2025-12-26T06:32:00.857296",
     "exception": false,
     "start_time": "2025-12-26T06:32:00.845682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "  def __init__(self, num_classes=7):\n",
    "      super().__init__()\n",
    "      self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   \n",
    "      self.bn1   = nn.BatchNorm2d(32)\n",
    "      self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  \n",
    "      self.bn2   = nn.BatchNorm2d(64)\n",
    "      self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) \n",
    "      self.bn3   = nn.BatchNorm2d(128)\n",
    "      self.pool  = nn.MaxPool2d(2)  \n",
    "      self.avgpool = nn.AdaptiveAvgPool2d((1,1)) \n",
    "      self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = F.relu(self.bn1(self.conv1(x)))\n",
    "      x = self.pool(x)                    \n",
    "      x = F.relu(self.bn2(self.conv2(x)))\n",
    "      x = self.pool(x)                    \n",
    "      x = F.relu(self.bn3(self.conv3(x)))\n",
    "      x = self.pool(x)                    \n",
    "      x = self.avgpool(x)                 \n",
    "      x = x.view(x.size(0), -1)           \n",
    "      x = self.fc(x)                      \n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16a5654b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T06:32:00.865212Z",
     "iopub.status.busy": "2025-12-26T06:32:00.864968Z",
     "iopub.status.idle": "2025-12-26T09:22:16.961866Z",
     "shell.execute_reply": "2025-12-26T09:22:16.960962Z"
    },
    "id": "MFl4XomSJY0k",
    "outputId": "756b5811-d77f-445c-9db8-ccd4a845b947",
    "papermill": {
     "duration": 10216.107919,
     "end_time": "2025-12-26T09:22:16.968579",
     "exception": false,
     "start_time": "2025-12-26T06:32:00.860660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Held-out: art_painting  (train: ['cartoon', 'photo', 'sketch']) ===\n",
      " - seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/887805067.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if (use_amp and self.device.startswith('cuda')) else None\n",
      "/tmp/ipykernel_24/887805067.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R0] avg_acc=0.1161 min_sim=None mean_sim=None applied_delta=False\n",
      "[R5] avg_acc=0.1708 min_sim=-0.5401227555127828 mean_sim=-0.21854780280502897 applied_delta=True\n",
      "[R10] avg_acc=0.1577 min_sim=-0.4035061536217562 mean_sim=-0.14166754539506068 applied_delta=True\n",
      "[R15] avg_acc=0.2703 min_sim=-0.24963612903130536 mean_sim=-0.08382816015524364 applied_delta=True\n",
      "[R20] avg_acc=0.2905 min_sim=-0.31645889774018593 mean_sim=-0.061105092788066716 applied_delta=False\n",
      "[R25] avg_acc=0.2573 min_sim=0.17201866520525622 mean_sim=0.2542439433411969 applied_delta=False\n",
      "[R30] avg_acc=0.2041 min_sim=-0.021244989134739573 mean_sim=0.0417372753053128 applied_delta=False\n",
      "[R35] avg_acc=0.2929 min_sim=0.25856767549120346 mean_sim=0.34705582299458365 applied_delta=False\n",
      "[R40] avg_acc=0.3242 min_sim=0.16765391130762072 mean_sim=0.19304065586933686 applied_delta=False\n",
      "[R45] avg_acc=0.2305 min_sim=0.1948631923566992 mean_sim=0.2675955667875534 applied_delta=False\n",
      "Saved model: /kaggle/working/fed_gga_full/final_model_art_painting_seed0.pt\n",
      " -> held_out art_painting seed 0 acc=0.1948 time=2564.5s\n",
      "\n",
      "=== Held-out: cartoon  (train: ['art_painting', 'photo', 'sketch']) ===\n",
      " - seed 0\n",
      "[R0] avg_acc=0.1470 min_sim=None mean_sim=None applied_delta=False\n",
      "[R5] avg_acc=0.2505 min_sim=-0.11813475822867936 mean_sim=0.08645511127212695 applied_delta=True\n",
      "[R10] avg_acc=0.3041 min_sim=-0.05358970587779328 mean_sim=0.021021420839518856 applied_delta=True\n",
      "[R15] avg_acc=0.2143 min_sim=-0.16027574642672324 mean_sim=0.059964061388870754 applied_delta=True\n",
      "[R20] avg_acc=0.2780 min_sim=-0.36475690507715636 mean_sim=-0.1797839579864603 applied_delta=False\n",
      "[R25] avg_acc=0.2875 min_sim=-0.3875365188793869 mean_sim=-0.21680501780305275 applied_delta=False\n",
      "[R30] avg_acc=0.3320 min_sim=-0.27705107932380296 mean_sim=-0.20789904552568364 applied_delta=False\n",
      "[R35] avg_acc=0.3016 min_sim=-0.28359629469617925 mean_sim=-0.2021249396542947 applied_delta=False\n",
      "[R40] avg_acc=0.2975 min_sim=-0.3448877280693464 mean_sim=-0.21453664018111782 applied_delta=False\n",
      "[R45] avg_acc=0.3288 min_sim=-0.374803203157365 mean_sim=-0.29059806982524633 applied_delta=False\n",
      "Saved model: /kaggle/working/fed_gga_full/final_model_cartoon_seed0.pt\n",
      " -> held_out cartoon seed 0 acc=0.2197 time=2518.5s\n",
      "\n",
      "=== Held-out: photo  (train: ['art_painting', 'cartoon', 'sketch']) ===\n",
      " - seed 0\n",
      "[R0] avg_acc=0.1814 min_sim=None mean_sim=None applied_delta=False\n",
      "[R5] avg_acc=0.1976 min_sim=-0.6780131430719141 mean_sim=-0.2688679006352674 applied_delta=True\n",
      "[R10] avg_acc=0.1867 min_sim=-0.4195936352899514 mean_sim=-0.17771870054674754 applied_delta=True\n",
      "[R15] avg_acc=0.1685 min_sim=-0.14892762933122825 mean_sim=0.02568696492238125 applied_delta=True\n",
      "[R20] avg_acc=0.2063 min_sim=-0.0455312627902277 mean_sim=0.09019252858331174 applied_delta=False\n",
      "[R25] avg_acc=0.1888 min_sim=-0.09403546271015253 mean_sim=0.024755211588960296 applied_delta=False\n",
      "[R30] avg_acc=0.1933 min_sim=0.0307272146918253 mean_sim=0.14328840536528029 applied_delta=False\n",
      "[R35] avg_acc=0.1814 min_sim=-0.031967209796773234 mean_sim=0.10757081645608702 applied_delta=False\n",
      "[R40] avg_acc=0.1771 min_sim=-0.10866487644212401 mean_sim=-0.020071447166883107 applied_delta=False\n",
      "[R45] avg_acc=0.2290 min_sim=-0.17797352804912392 mean_sim=-0.01162241296915143 applied_delta=False\n",
      "Saved model: /kaggle/working/fed_gga_full/final_model_photo_seed0.pt\n",
      " -> held_out photo seed 0 acc=0.2635 time=2578.4s\n",
      "\n",
      "=== Held-out: sketch  (train: ['art_painting', 'cartoon', 'photo']) ===\n",
      " - seed 0\n",
      "[R0] avg_acc=0.2440 min_sim=None mean_sim=None applied_delta=False\n",
      "[R5] avg_acc=0.2638 min_sim=-0.26824746281122996 mean_sim=-0.019549760937972744 applied_delta=True\n",
      "[R10] avg_acc=0.2603 min_sim=-0.16233319257644488 mean_sim=-0.043498195875475855 applied_delta=True\n",
      "[R15] avg_acc=0.2529 min_sim=0.03281338189283168 mean_sim=0.12670514182552536 applied_delta=True\n",
      "[R20] avg_acc=0.2308 min_sim=-0.008867620957078305 mean_sim=0.19426228939884316 applied_delta=False\n",
      "[R25] avg_acc=0.2154 min_sim=-0.0935736606868636 mean_sim=0.21863231174373246 applied_delta=False\n",
      "[R30] avg_acc=0.2493 min_sim=-0.14061253270274843 mean_sim=0.17803918170327734 applied_delta=False\n",
      "[R35] avg_acc=0.2842 min_sim=-0.17333909277082166 mean_sim=0.17379487920698775 applied_delta=False\n",
      "[R40] avg_acc=0.2610 min_sim=0.046383835156905195 mean_sim=0.27057567326971105 applied_delta=False\n",
      "[R45] avg_acc=0.2560 min_sim=-0.2772907057999678 mean_sim=-0.008210175749371412 applied_delta=False\n",
      "Saved model: /kaggle/working/fed_gga_full/final_model_sketch_seed0.pt\n",
      " -> held_out sketch seed 0 acc=0.1547 time=2533.3s\n",
      "\n",
      "Average accuracy across held-out domains: 0.20818848128815068\n",
      "\n",
      "Saved results to: /kaggle/working/fed_gga_full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/16260683.py:123: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  table_row[d] = f\"{float(r['mean']):.3f} ± {float(r['std']):.3f}\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "def eval_global_on_test(model, test_loader, device):\n",
    "    model = deepcopy(model).to(device)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb).argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    seed = int(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def run_heldout_experiments(domain_loaders,\n",
    "                            seeds=[0],\n",
    "                            optimizer='adam',\n",
    "                            save_dir='/kaggle/working/fed_gga_results'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    domains = list(domain_loaders.keys())\n",
    "    num_classes = 7\n",
    "\n",
    "    results = []\n",
    "    for held_out in domains:\n",
    "        train_domains = [d for d in domains if d != held_out]\n",
    "        print(f\"\\n=== Held-out: {held_out}  (train: {train_domains}) ===\")\n",
    "\n",
    "        for seed in seeds:\n",
    "            set_global_seed(seed)\n",
    "            print(f\" - seed {seed}\")\n",
    "            # build clients\n",
    "            clients = []\n",
    "            for d in train_domains:\n",
    "                clients.append(FedClient(d, domain_loaders[d], domain_loaders[d], device))\n",
    "\n",
    "            # model = make_model(name=model_name, num_classes=num_classes).to(device)\n",
    "            model=SmallCNN(num_classes=7).to(device)\n",
    "            \n",
    "            cfg = {}\n",
    "            cfg.setdefault('rounds', 50)\n",
    "            cfg.setdefault('R_start', 2)\n",
    "            cfg.setdefault('R_end', 15)\n",
    "            cfg.setdefault('D_start', 20)\n",
    "            cfg.setdefault('P_start', 42)\n",
    "            cfg.setdefault('P_tolerance', 0.2)\n",
    "            cfg.setdefault('P_patience', 1)\n",
    "            cfg.setdefault('K', 8)\n",
    "            cfg.setdefault('rho', 1e-5)\n",
    "            cfg.setdefault('beta', 0.3)\n",
    "            cfg.setdefault('local_epochs', 2)\n",
    "            cfg.setdefault('max_client_steps', 100)\n",
    "            cfg.setdefault('local_lr',  1e-3)\n",
    "            cfg.setdefault('server_lr', 1e-3)\n",
    "            cfg.setdefault('enable_gga', True)\n",
    "            cfg.setdefault('enable_dampening', True)\n",
    "            cfg.setdefault('enable_pruning', True)\n",
    "            cfg.setdefault('use_amp', True)\n",
    "            cfg.setdefault('scale_by_norm', True)\n",
    "            cfg.setdefault('search_last_layer_only', False)\n",
    "            cfg.setdefault('loss_relax', 0.05)\n",
    "\n",
    "            held_test_loader = domain_loaders[held_out]\n",
    "            server = FedGGAServer(model, clients, device, cfg, test_loader=held_test_loader)\n",
    "\n",
    "            t0 = time.time()\n",
    "            run_log = server.run()\n",
    "            run_time = time.time() - t0\n",
    "\n",
    "            # held-out evaluation\n",
    "            held_client = FedClient(held_out, None, held_test_loader, device)\n",
    "            held_out_acc = held_client.eval_on_test(server.model)\n",
    "\n",
    "            run_id = f\"{held_out}_seed{seed}\"\n",
    "            pd.DataFrame(run_log).to_csv(os.path.join(save_dir, f\"runlog_{run_id}.csv\"), index=False)\n",
    "            \n",
    "            model_path = os.path.join(save_dir, f\"final_model_{run_id}.pt\")\n",
    "            torch.save(server.model.state_dict(), model_path)\n",
    "            print(\"Saved model:\", model_path)\n",
    "\n",
    "            results.append({\n",
    "                'dataset': 'PACS',\n",
    "                'held_out': held_out,\n",
    "                'seed': seed,\n",
    "                'optimizer': optimizer,\n",
    "                'held_out_acc': held_out_acc,\n",
    "                'time_s': run_time,\n",
    "                'cfg_K': cfg['K'],\n",
    "                'cfg_rho': cfg['rho'],\n",
    "                'cfg_R_start': cfg['R_start'],\n",
    "                'cfg_R_end': cfg['R_end'],\n",
    "            })\n",
    "\n",
    "            print(f\" -> held_out {held_out} seed {seed} acc={held_out_acc:.4f} time={run_time:.1f}s\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(save_dir, \"per_run_results.csv\"), index=False)\n",
    "    \n",
    "    domain_means = df.groupby('held_out')['held_out_acc'].mean()\n",
    "    avg_across_domains = domain_means.mean()\n",
    "    \n",
    "    print(\"\\nAverage accuracy across held-out domains:\", avg_across_domains)\n",
    "\n",
    "    summary = df.groupby('held_out')['held_out_acc'].agg(['mean','std']).reset_index()\n",
    "    dataset_avg = summary['mean'].mean()\n",
    "    summary.to_csv(os.path.join(save_dir, \"per_domain_summary.csv\"), index=False)\n",
    "\n",
    "    # table-like CSV\n",
    "    pacs_order = domains\n",
    "    table_row = {}\n",
    "    for d in pacs_order:\n",
    "        r = summary[summary['held_out'] == d]\n",
    "        if len(r) > 0:\n",
    "            table_row[d] = f\"{float(r['mean']):.3f} ± {float(r['std']):.3f}\"\n",
    "        else:\n",
    "            table_row[d] = \"N/A\"\n",
    "    table_row['Avg'] = f\"{float(dataset_avg):.3f}\"\n",
    "    pd.DataFrame([table_row]).to_csv(os.path.join(save_dir, \"pacs_table_row.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nSaved results to:\", save_dir)\n",
    "    return df, summary, table_row, server.model\n",
    "\n",
    "# df_full, summary_full, table_full = run_heldout_experiments(domain_loaders, model_name='resnet50', cfg_base=cfg_full, seeds=[0,1,2], optimizer='adam', save_dir='/kaggle/working/fed_gga_full')\n",
    "df_full, summary_full, table_full, trained_model = run_heldout_experiments(domain_loaders, seeds=[0], optimizer='adam', save_dir='/kaggle/working/fed_gga_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695b1de7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T09:22:16.980146Z",
     "iopub.status.busy": "2025-12-26T09:22:16.979888Z",
     "iopub.status.idle": "2025-12-26T09:22:16.996952Z",
     "shell.execute_reply": "2025-12-26T09:22:16.996255Z"
    },
    "papermill": {
     "duration": 0.024852,
     "end_time": "2025-12-26T09:22:16.998299",
     "exception": false,
     "start_time": "2025-12-26T09:22:16.973447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_runlog_file(path):\n",
    "    \"\"\"Load CSV runlog safely and normalize round column.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Runlog not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    if 'round' in df.columns:\n",
    "        df['round'] = df['round'].astype(int)\n",
    "        df = df.sort_values('round').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _maybe_smooth(series, window):\n",
    "    if window is None or window <= 1:\n",
    "        return series\n",
    "    return series.rolling(window=window, min_periods=1, center=False).mean()\n",
    "\n",
    "def plot_all_heldout_global_curves(runlog_paths, labels=None, save_path=None, smooth_window=None, figsize=(9,5)):\n",
    "    \"\"\"\n",
    "    Plot per-held-out global_accu curves.\n",
    "      runlog_paths: list of file paths\n",
    "      labels: optional list of labels for legend\n",
    "      smooth_window: int or None (rolling smoothing)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    any_plotted = False\n",
    "    for i, p in enumerate(runlog_paths):\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"warning: runlog missing {p}; skipping\")\n",
    "            continue\n",
    "        df = load_runlog_file(p)\n",
    "        if 'avg_client_acc' not in df.columns:\n",
    "            print(f\"warning: 'avg_client_acc' not found in {p}; skipping\")\n",
    "            continue\n",
    "        lab = (labels[i] if labels is not None and i < len(labels) else os.path.basename(p))\n",
    "        y = pd.to_numeric(df['avg_client_acc'], errors='coerce')\n",
    "        y = y.fillna(method='ffill').fillna(method='bfill')  # try to fill edges\n",
    "        # y = _maybe_smooth(y, smooth_window)\n",
    "        plt.plot(df['round'], y, marker='o', label=lab)\n",
    "        any_plotted = True\n",
    "\n",
    "    if not any_plotted:\n",
    "        print(\"No avg_client_acc curves plotted (no files/columns found).\")\n",
    "        return\n",
    "\n",
    "    plt.xlabel('round')\n",
    "    plt.ylabel('held-out / global accuracy')\n",
    "    plt.title('Held-out accuracy per round')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        print(\"Saved\", save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_similarity_across_domains(runlog_paths_dict, save_path=None, smooth_window=None, figsize=(9,5)):\n",
    "    \"\"\"\n",
    "    Plot mean_pairwise_sim across multiple domains.\n",
    "      runlog_paths_dict: dict domain -> path\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    any_plotted = False\n",
    "    for domain, path in runlog_paths_dict.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"warning: runlog missing {path}; skipping {domain}\")\n",
    "            continue\n",
    "        df = load_runlog_file(path)\n",
    "        if 'mean_pairwise_sim' not in df.columns:\n",
    "            print(f\"warning: 'mean_pairwise_sim' not found in {path}; skipping {domain}\")\n",
    "            continue\n",
    "        series = pd.to_numeric(df['mean_pairwise_sim'], errors='coerce')\n",
    "        # fill NaNs sensibly: forward then backward\n",
    "        series = series.fillna(method='ffill').fillna(method='bfill')\n",
    "        # series = _maybe_smooth(series, smooth_window)\n",
    "        plt.plot(df['round'], series, marker='o', label=domain)\n",
    "        any_plotted = True\n",
    "\n",
    "    if not any_plotted:\n",
    "        print(\"No similarity curves plotted (no files/columns found).\")\n",
    "        return\n",
    "\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Mean pairwise similarity')\n",
    "    plt.title('Mean pairwise similarity (ref grads) per round')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        print(\"Saved\", save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_global_and_similarity(runlog_paths_dict, save_path=None, smooth_window=None, figsize=(12,5)):\n",
    "    \"\"\"\n",
    "    Two-panel plot: left = avg_client_acc per-domain, right = mean_pairwise_sim per-domain.\n",
    "    runlog_paths_dict: dict label -> path\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    any_left = any_right = False\n",
    "\n",
    "    for label, path in runlog_paths_dict.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"warning: {path} missing; skipping {label}\")\n",
    "            continue\n",
    "        df = load_runlog_file(path)\n",
    "        rounds = df['round'] if 'round' in df.columns else np.arange(len(df))\n",
    "        if 'avg_client_acc' in df.columns:\n",
    "            ga = pd.to_numeric(df['avg_client_acc'], errors='coerce').fillna(method='ffill').fillna(method='bfill')\n",
    "            # ga = _maybe_smooth(ga, smooth_window)\n",
    "            ax1.plot(rounds, ga, marker='o', label=label)\n",
    "            any_left = True\n",
    "        if 'mean_pairwise_sim' in df.columns:\n",
    "            ms = pd.to_numeric(df['mean_pairwise_sim'], errors='coerce').fillna(method='ffill').fillna(method='bfill')\n",
    "            # ms = _maybe_smooth(ms, smooth_window)\n",
    "            ax2.plot(rounds, ms, marker='o', label=label)\n",
    "            any_right = True\n",
    "\n",
    "    if any_left:\n",
    "        ax1.set_xlabel('round'); ax1.set_ylabel('avg_client_acc'); ax1.set_title('Held-out/global accuracy')\n",
    "        ax1.grid(True); ax1.legend()\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No avg_client_acc data', ha='center', va='center'); ax1.axis('off')\n",
    "\n",
    "    if any_right:\n",
    "        ax2.set_xlabel('round'); ax2.set_ylabel('mean_pairwise_sim'); ax2.set_title('Mean pairwise similarity')\n",
    "        ax2.grid(True); ax2.legend()\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No similarity data', ha='center', va='center'); ax2.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        print(\"Saved\", save_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d84058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T09:22:17.008948Z",
     "iopub.status.busy": "2025-12-26T09:22:17.008614Z",
     "iopub.status.idle": "2025-12-26T09:22:17.189921Z",
     "shell.execute_reply": "2025-12-26T09:22:17.189306Z"
    },
    "papermill": {
     "duration": 0.188121,
     "end_time": "2025-12-26T09:22:17.191245",
     "exception": false,
     "start_time": "2025-12-26T09:22:17.003124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: runlog missing /kaggle/working/fed_gga_results/runlog_photo_seed0.csv; skipping\n",
      "warning: runlog missing /kaggle/working/fed_gga_results/runlog_art_painting_seed0.csv; skipping\n",
      "warning: runlog missing /kaggle/working/fed_gga_results/runlog_cartoon_seed0.csv; skipping\n",
      "warning: runlog missing /kaggle/working/fed_gga_results/runlog_sketch_seed0.csv; skipping\n",
      "No avg_client_acc curves plotted (no files/columns found).\n",
      "warning: runlog missing /kaggle/working/fed_gga_results/runlog_photo_seed0.csv; skipping photo\n",
      "warning: runlog missing /kaggle/working/fed_gga_results/runlog_art_painting_seed0.csv; skipping art_painting\n",
      "warning: runlog missing /kaggle/working/fed_gga_results/runlog_cartoon_seed0.csv; skipping cartoon\n",
      "warning: runlog missing /kaggle/working/fed_gga_results/runlog_sketch_seed0.csv; skipping sketch\n",
      "No similarity curves plotted (no files/columns found).\n",
      "warning: /kaggle/working/fed_gga_results/runlog_photo_seed0.csv missing; skipping photo\n",
      "warning: /kaggle/working/fed_gga_results/runlog_art_painting_seed0.csv missing; skipping art_painting\n",
      "warning: /kaggle/working/fed_gga_results/runlog_cartoon_seed0.csv missing; skipping cartoon\n",
      "warning: /kaggle/working/fed_gga_results/runlog_sketch_seed0.csv missing; skipping sketch\n",
      "Saved /kaggle/working/fed_gga_results/global_and_similarity.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base = '/kaggle/working/fed_gga_results'  # update if needed\n",
    "held_outs = ['photo','art_painting','cartoon','sketch']\n",
    "paths = [os.path.join(base, f\"runlog_{h}_seed0.csv\") for h in held_outs]\n",
    "\n",
    "# simple global curves\n",
    "plot_all_heldout_global_curves(paths, labels=held_outs,\n",
    "                               save_path=os.path.join(base,'heldout_global_curves.png'),\n",
    "                               smooth_window=1)\n",
    "\n",
    "# similarity across domains: pass dict domain->path\n",
    "runlog_dict = {h: os.path.join(base, f\"runlog_{h}_seed0.csv\") for h in held_outs}\n",
    "plot_similarity_across_domains(runlog_dict,\n",
    "                              save_path=os.path.join(base,'similarity_across_domains.png'),\n",
    "                              smooth_window=1)\n",
    "\n",
    "# combined view\n",
    "plot_global_and_similarity(runlog_dict,\n",
    "                           save_path=os.path.join(base,'global_and_similarity.png'),\n",
    "                           smooth_window=3)   # use small smoothing for nicer curves\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10264.425589,
   "end_time": "2025-12-26T09:22:20.208951",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-26T06:31:15.783362",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00e8be77f1e1402b8465201b1898ec37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0339b4924234483aafd069fa1d343d4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "05dc6ca9b10445f2a252be7241c8f2ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08a808d655f84f33a4a173d940c7dc70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3e11e4665d7a466dae86c04ed84b93d9",
       "max": 9991.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2e69a805a57547aaa46004db44453347",
       "tabbable": null,
       "tooltip": null,
       "value": 9991.0
      }
     },
     "1342b6da05c147f4bbb23d4a1a70d83d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e39896f262844b2c872d3082348adeba",
       "max": 191395900.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_778d36047c5545e89975d4f495aa6fe3",
       "tabbable": null,
       "tooltip": null,
       "value": 191395900.0
      }
     },
     "15c435467c6f44a398360d66e66c404d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2f8b45789ff5435a9e056e2fae0d254a",
       "placeholder": "​",
       "style": "IPY_MODEL_95a302f8d5794785a7a76c6b4aea2859",
       "tabbable": null,
       "tooltip": null,
       "value": "data/train-00000-of-00001.parquet: 100%"
      }
     },
     "1897b34d405b4ffdb358d07ce51537d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2401ca280d5348ffab56f93f5d2e12a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c6dbc34005249c2ad5f4fed68de0745": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2e69a805a57547aaa46004db44453347": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2e9c5616d61b41deb9d5065968a96b69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ee6011e65ece45a8b4afcb53d44fa3f1",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2c6dbc34005249c2ad5f4fed68de0745",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "2f8b45789ff5435a9e056e2fae0d254a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35c3894772bc46c3b31e91faa4df1130": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b154397ccaf4c08a4055b7465826ec8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3e11e4665d7a466dae86c04ed84b93d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3fff9f27209f4166b57f53fcf00948f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ce01520e22c477b99cc670a7bbf7b48",
        "IPY_MODEL_08a808d655f84f33a4a173d940c7dc70",
        "IPY_MODEL_c2a680ea35ed42b6b21008e1cb686d47"
       ],
       "layout": "IPY_MODEL_bcdadb12d1e149509f0a3d6882bb41ca",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4a70b86bfd6a49c59817694db76069f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_daba8f44c15441b89d2072b7e63c9aba",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_00e8be77f1e1402b8465201b1898ec37",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "5322f0ba672146638ee216f07489141a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2401ca280d5348ffab56f93f5d2e12a4",
       "placeholder": "​",
       "style": "IPY_MODEL_e6cb47b8c3e94fc9a96eee9bba1a49c0",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.89k/? [00:00&lt;00:00, 346kB/s]"
      }
     },
     "5460673755a147f2807e2e252248fcd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8fb5098ecd1d44c687cc2fba9ff4b19c",
       "placeholder": "​",
       "style": "IPY_MODEL_7a09eb1a119c4ff3813fac5d8555de08",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing checksums: 100%"
      }
     },
     "67249b766e294e18ac07dfe5f0387f33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "778d36047c5545e89975d4f495aa6fe3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7a09eb1a119c4ff3813fac5d8555de08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7ce01520e22c477b99cc670a7bbf7b48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_95695f97eff24faa9eb93a8108ac4dad",
       "placeholder": "​",
       "style": "IPY_MODEL_3b154397ccaf4c08a4055b7465826ec8",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating train split: 100%"
      }
     },
     "7ddb4741559b47538d4f5a0af7bbfa31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d6b9516ab26f4a509a1127111eb70666",
        "IPY_MODEL_4a70b86bfd6a49c59817694db76069f7",
        "IPY_MODEL_5322f0ba672146638ee216f07489141a"
       ],
       "layout": "IPY_MODEL_ed1c814554664e8ba7d04ffe31ff9fd0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7ff27610776b4b7fa717a1d35a764bee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a5cb179f5284720b8fb239287e4cfb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8fb5098ecd1d44c687cc2fba9ff4b19c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95695f97eff24faa9eb93a8108ac4dad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95a302f8d5794785a7a76c6b4aea2859": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a1987314741b4dfe920d47aec3a4e66d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a29b635c61184bdeb18f1574b086f4b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a63293c3709f4eb4804cd790f6c9e3c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b98bb66e791844bc9e654b91f34075b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5460673755a147f2807e2e252248fcd4",
        "IPY_MODEL_2e9c5616d61b41deb9d5065968a96b69",
        "IPY_MODEL_f9b1a72a64be4b838dd3974fdff4fff8"
       ],
       "layout": "IPY_MODEL_7ff27610776b4b7fa717a1d35a764bee",
       "tabbable": null,
       "tooltip": null
      }
     },
     "bcdadb12d1e149509f0a3d6882bb41ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c26dd4289cfb427f80f6d3ccf5142d6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15c435467c6f44a398360d66e66c404d",
        "IPY_MODEL_1342b6da05c147f4bbb23d4a1a70d83d",
        "IPY_MODEL_c962e2fc2f5446f989e3d51512e781d3"
       ],
       "layout": "IPY_MODEL_1897b34d405b4ffdb358d07ce51537d8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c2a680ea35ed42b6b21008e1cb686d47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a63293c3709f4eb4804cd790f6c9e3c4",
       "placeholder": "​",
       "style": "IPY_MODEL_a1987314741b4dfe920d47aec3a4e66d",
       "tabbable": null,
       "tooltip": null,
       "value": " 9991/9991 [00:00&lt;00:00, 33371.18 examples/s]"
      }
     },
     "c962e2fc2f5446f989e3d51512e781d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_35c3894772bc46c3b31e91faa4df1130",
       "placeholder": "​",
       "style": "IPY_MODEL_a29b635c61184bdeb18f1574b086f4b4",
       "tabbable": null,
       "tooltip": null,
       "value": " 191M/191M [00:01&lt;00:00, 45.0MB/s]"
      }
     },
     "d6b9516ab26f4a509a1127111eb70666": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0339b4924234483aafd069fa1d343d4f",
       "placeholder": "​",
       "style": "IPY_MODEL_67249b766e294e18ac07dfe5f0387f33",
       "tabbable": null,
       "tooltip": null,
       "value": "README.md: "
      }
     },
     "daba8f44c15441b89d2072b7e63c9aba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "e39896f262844b2c872d3082348adeba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6cb47b8c3e94fc9a96eee9bba1a49c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ed1c814554664e8ba7d04ffe31ff9fd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee6011e65ece45a8b4afcb53d44fa3f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f9b1a72a64be4b838dd3974fdff4fff8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_05dc6ca9b10445f2a252be7241c8f2ff",
       "placeholder": "​",
       "style": "IPY_MODEL_8a5cb179f5284720b8fb239287e4cfb7",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 168.24it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
