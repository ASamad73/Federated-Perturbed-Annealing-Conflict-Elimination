{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14285302,"sourceType":"datasetVersion","datasetId":9118013},{"sourceId":14309766,"sourceType":"datasetVersion","datasetId":9135154},{"sourceId":14309768,"sourceType":"datasetVersion","datasetId":9135156},{"sourceId":14309773,"sourceType":"datasetVersion","datasetId":9135161},{"sourceId":14314690,"sourceType":"datasetVersion","datasetId":9138183},{"sourceId":14314695,"sourceType":"datasetVersion","datasetId":9138185},{"sourceId":14314699,"sourceType":"datasetVersion","datasetId":9138188},{"sourceId":14320882,"sourceType":"datasetVersion","datasetId":9142073},{"sourceId":14354231,"sourceType":"datasetVersion","datasetId":9165710}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 0) Environment setup (run on Colab)\n# If you start a fresh Colab runtime you can uncomment below to install torch if necessary:\n# !pip install torch torchvision --quiet\n# --- Standard Library ---\nimport os\nimport time\nimport math\nimport random\nfrom copy import deepcopy\nfrom functools import partial\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional, Tuple\n\n# --- Third-Party Data & Plotting ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# --- PyTorch Core ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.utils import parameters_to_vector, vector_to_parameters\nfrom torch.cuda.amp import autocast, GradScaler\n\n# --- PyTorch Data Utilities ---\nfrom torch.utils.data import Dataset, DataLoader, Subset, random_split\n\n# --- Torchvision ---\nimport torchvision\nimport torchvision.transforms as T\nimport torchvision.datasets as datasets\nfrom torchvision.models import resnet18, resnet50\n\n# Note: 'CIFAR10' is available via 'datasets.CIFAR10'\nfrom torchvision.datasets import CIFAR10\n\nseed = 0\\\n\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n  torch.cuda.manual_seed_all(seed)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Device:', device)","metadata":{"id":"rXh2D0ZW3ywP","outputId":"707e062d-b5d0-47da-d796-5730f03cefef","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_global_on_test(model, test_loader, device):\n    model = deepcopy(model).to(device)\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for xb, yb in test_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = model(xb).argmax(dim=1)\n            correct += (preds == yb).sum().item()\n            total += yb.size(0)\n    return correct / total if total > 0 else 0.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) Utilities: flattening, cosine similarity, pairwise stats\ndef flat_params_from_model(model):\n  return parameters_to_vector([p for p in model.parameters() if p.requires_grad]).detach().cpu()\n\ndef set_flat_params_to_model(model, flat_vec):\n  if isinstance(flat_vec, (list, np.ndarray)):\n    flat_vec = torch.from_numpy(np.array(flat_vec))\n  vector_to_parameters(flat_vec.to(next(model.parameters()).device), [p for p in model.parameters() if p.requires_grad])\n\ndef cosine_sim_np(a, b):\n  a = np.asarray(a, dtype=float).ravel()\n  b = np.asarray(b, dtype=float).ravel()\n  denom = (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12)\n  return float(np.dot(a, b) / denom)\n\ndef pairwise_cosine_stats(list_of_flat_grads):\n  n = len(list_of_flat_grads)\n  sims = []\n  for i in range(n):\n    for j in range(i+1, n):\n      sims.append(cosine_sim_np(list_of_flat_grads[i], list_of_flat_grads[j]))\n  if len(sims) == 0:\n    return 0.0, 0.0\n  return float(np.min(sims)), float(np.mean(sims))","metadata":{"id":"PLZbPrR_3_kp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2) Fed-GGA utilities (seed-based perturbations & client scoring)\ndef sample_k_seeds(K, base_seed=None):\n  rng = np.random.RandomState(base_seed)\n  return [int(rng.randint(0, 2**31 - 1)) for _ in range(K)]\n\n\ndef get_heldout_split(domain_loaders, held_out):\n  assert held_out in domain_loaders, f\"{held_out} is not a valid domain!\"\n\n  test_loader = domain_loaders[held_out]\n  train_loaders = [dl for name, dl in domain_loaders.items() if name != held_out]\n\n  return train_loaders, test_loader, held_out","metadata":{"id":"l0gYJ5hP4IgP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_global_seed(seed: int):\n    seed = int(seed)\n\n    # Python\n    random.seed(seed)\n\n    # NumPy\n    np.random.seed(seed)\n\n    # PyTorch (CPU)\n    torch.manual_seed(seed)\n\n    # PyTorch (CUDA)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n\n    # Ensure deterministic behavior (may slightly reduce performance)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Gradient Abstraction\n\n@dataclass\nclass GradientDict:\n    \"\"\"Encapsulates gradients as a dictionary of tensors.\"\"\"\n    gradients: Dict[str, torch.Tensor]\n    client_id: str\n    \n    def sign(self) -> Dict[str, torch.Tensor]:\n        \"\"\"Returns the sign of each gradient component.\"\"\"\n        return {name: torch.sign(grad) for name, grad in self.gradients.items()}\n    \n    def to_device(self, device):\n        \"\"\"Move all gradients to device.\"\"\"\n        self.gradients = {name: grad.to(device) for name, grad in self.gradients.items()}\n        return self\n    \n    @staticmethod\n    def from_model(model: nn.Module, client_id: str) -> 'GradientDict':\n        \"\"\"Extract gradients from model parameters.\"\"\"\n        gradients = {}\n        for name, param in model.named_parameters():\n            if param.grad is not None:\n                gradients[name] = param.grad.clone().detach()\n        return GradientDict(gradients=gradients, client_id=client_id)\n    \n    def apply_to_model(self, model: nn.Module, learning_rate: float):\n        \"\"\"Apply gradients to model parameters (gradient descent step).\"\"\"\n        with torch.no_grad():\n            for name, param in model.named_parameters():\n                if name in self.gradients:\n                    param.data -= learning_rate * self.gradients[name]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AggregationStrategy(ABC):\n    \"\"\"Abstract base class for gradient aggregation strategies.\"\"\"\n    \n    @abstractmethod\n    def aggregate(self, gradients: List[GradientDict]) -> Dict[str, torch.Tensor]:\n        \"\"\"Aggregate gradients from multiple clients.\"\"\"\n        pass\n\n\nclass AgreementWeightedFedAvg(AggregationStrategy):\n    \"\"\"\n    Agreement-Weighted Federated Averaging.\n    \n    Computes per-parameter agreement weights based on sign consensus:\n    W_j = |Σ sign((g_i)_j)| / N\n    \n    Final update: g_avg ⊙ W (element-wise multiplication)\n    \"\"\"\n    \n    def __init__(self, verbose: bool = False):\n        self.verbose = verbose\n        self.last_agreement_weights = {}\n    \n    def compute_average_gradient(\n        self, \n        gradients: List[GradientDict]\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Compute g_avg = (1/N) * Σ g_i\"\"\"\n        n = len(gradients)\n        averaged = {}\n        \n        param_names = gradients[0].gradients.keys()\n        \n        for name in param_names:\n            grad_sum = sum(g.gradients[name] for g in gradients)\n            averaged[name] = grad_sum / n\n        \n        return averaged\n    \n    def compute_agreement_weights(\n        self, \n        gradients: List[GradientDict]\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute per-parameter agreement weights:\n        W_j = |Σ sign((g_i)_j)| / N\n        \"\"\"\n        n = len(gradients)\n        weights = {}\n        \n        param_names = gradients[0].gradients.keys()\n        \n        for name in param_names:\n            # Sum of signs for each parameter across all clients\n            sign_sum = sum(torch.sign(g.gradients[name]) for g in gradients)\n            \n            # Absolute value normalized by number of clients\n            weights[name] = torch.abs(sign_sum) / n\n        \n        return weights\n    \n    def aggregate(self, gradients: List[GradientDict]) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Aggregate gradients using agreement weighting.\n        \n        Returns: g_avg ⊙ W\n        \"\"\"\n        if not gradients:\n            raise ValueError(\"Cannot aggregate empty gradient list\")\n        \n        # Step 1: Compute average gradient\n        g_avg = self.compute_average_gradient(gradients)\n        \n        # Step 2: Compute agreement weights\n        weights = self.compute_agreement_weights(gradients)\n        self.last_agreement_weights = weights\n        \n        # Step 3: Apply element-wise multiplication\n        weighted_gradient = {\n            name: g_avg[name] * weights[name]\n            for name in g_avg.keys()\n        }\n        \n        if self.verbose:\n            for name in weights.keys():\n                w = weights[name]\n                print(f\"  {name}: Agreement - Min: {w.min():.3f}, \"\n                      f\"Max: {w.max():.3f}, Mean: {w.mean():.3f}\")\n        \n        return weighted_gradient\n    \n    def get_agreement_statistics(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"Get statistics about the last computed agreement weights.\"\"\"\n        stats = {}\n        for name, weight in self.last_agreement_weights.items():\n            stats[name] = {\n                \"min\": float(weight.min()),\n                \"max\": float(weight.max()),\n                \"mean\": float(weight.mean()),\n                \"std\": float(weight.std())\n            }\n        return stats","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PruningFedAvg(AggregationStrategy):\n    def __init__(self, threshold: float = 0.3, patience: int = 5):\n        self.threshold = threshold\n        self.patience = patience\n        self.pruning_rate = 0.0\n        # Memory to store how many consecutive rounds a parameter has been in conflict\n        self.consecutive_conflict_counts = {} \n        self.last_agreement_weights = {}\n\n    def compute_agreement_weights(self, gradients: List[GradientDict]):\n        n_clients = len(gradients)\n\n        weights = {}\n        for gradient in gradients:\n            for name, grad in gradient.gradients.items():\n                if name not in weights:\n                    weights[name] = torch.zeros_like(grad)\n                weights[name] += torch.sign(grad)\n        for name in weights:\n            weights[name] = torch.abs(weights[name]) / n_clients\n        return weights\n\n    def aggregate(self, gradients: List[GradientDict]):\n        N = len(gradients)\n        param_names = gradients[0].gradients.keys()\n        \n        # 1. Calculate standard average gradient\n        g_avg = {name: sum(g.gradients[name] for g in gradients) / N for name in param_names}\n\n        weights = self.compute_agreement_weights(gradients)\n\n        self.last_agreement_weights = weights\n        \n        for name in g_avg.keys():\n            g_avg[name] = g_avg[name] * weights[name]\n\n        pruned_grads = {}\n        total_params, pruned_params = 0, 0\n        \n        for name in param_names:\n            # 2. Calculate Agreement W_j\n            sign_sum = sum(torch.sign(g.gradients[name]) for g in gradients)\n            agreement = torch.abs(sign_sum) / N\n            \n            # Initialize counter for this parameter layer if it doesn't exist\n            if name not in self.consecutive_conflict_counts:\n                self.consecutive_conflict_counts[name] = torch.zeros_like(agreement)\n\n            # 3. Check for conflict: If agreement < threshold, increment count. Else, RESET to 0.\n            # (agreement < self.threshold) creates a boolean mask\n            has_conflict = (agreement < self.threshold)\n            \n            # Increment where there is conflict\n            self.consecutive_conflict_counts[name] += has_conflict.float()\n            \n            # IMPORTANT: Reset count to 0 for any parameter that NOW reaches consensus\n            self.consecutive_conflict_counts[name] *= has_conflict.float()\n\n            # 4. Generate the Pruning Mask\n            # Mask is 0 only if the conflict count has reached the 'patience' limit (e.g., 5)\n            mask = (self.consecutive_conflict_counts[name] < self.patience).float()\n            \n            # 5. Apply Pruning\n            pruned_grads[name] = g_avg[name] * mask\n            \n            # Track statistics\n            total_params += mask.numel()\n            pruned_params += (mask == 0).sum().item()\n            \n        pruning_rate = (pruned_params / total_params) * 100\n        self.pruning_rate = pruning_rate\n        return pruned_grads, pruning_rate\n\n\n    def get_agreement_statistics(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"Get statistics about the last computed agreement weights.\"\"\"\n        stats = {}\n        for name, weight in self.last_agreement_weights.items():\n            stats[name] = {\n                \"min\": float(weight.min()),\n                \"max\": float(weight.max()),\n                \"mean\": float(weight.mean()),\n                \"std\": float(weight.std())\n            }\n        return stats","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imagenet_mean = (0.485, 0.456, 0.406)\nimagenet_std  = (0.229, 0.224, 0.225)\n\ntrain_transform_cifar = T.Compose([\n    T.Resize(256),\n    T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    T.RandomHorizontalFlip(),\n    T.ToTensor(),\n    T.Normalize(imagenet_mean, imagenet_std)\n])\ntest_transform_cifar = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(imagenet_mean, imagenet_std)\n])\n\n\ndef dirichlet_partition_noniid(dataset_targets, num_clients, alpha, min_size=10, rng=None):\n    \"\"\"\n    Partition indices of dataset_targets (list/np.array of class labels) into `num_clients` parts\n    using Dirichlet concentration alpha (per-class distribution across clients).\n    Returns a list of index lists: indices_per_client[k] = list of dataset indices for client k.\n    \"\"\"\n    if rng is None:\n        rng = np.random.RandomState(1234)\n    labels = np.array(dataset_targets)\n    num_classes = int(labels.max()) + 1\n    idx_by_class = [np.where(labels == c)[0].tolist() for c in range(num_classes)]\n    indices_per_client = [[] for _ in range(num_clients)]\n\n    for c in range(num_classes):\n        idx_c = idx_by_class[c]\n        if len(idx_c) == 0:\n            continue\n        # draw proportions\n        proportions = rng.dirichlet([alpha] * num_clients)\n        # Convert proportions to counts; ensure at least one sample per client sometimes\n        # shuffle indices\n        rng.shuffle(idx_c)\n        # compute split sizes\n        counts = (proportions * len(idx_c)).astype(int)\n        # fix rounding errors: ensure sum(counts) == len(idx_c)\n        diff = len(idx_c) - np.sum(counts)\n        while diff > 0:\n            # add one to the client with largest fractional remainder\n            frac = proportions * len(idx_c) - counts\n            idx = int(np.argmax(frac))\n            counts[idx] += 1\n            diff -= 1\n        # assign\n        pointer = 0\n        for k in range(num_clients):\n            cnt = counts[k]\n            if cnt > 0:\n                portion = idx_c[pointer:pointer+cnt]\n                indices_per_client[k].extend(portion)\n                pointer += cnt\n\n    # Ensure minimum size per client (simple repair: move random samples if needed)\n    for k in range(num_clients):\n        if len(indices_per_client[k]) < min_size:\n            # collect donors\n            donors = [j for j in range(num_clients) if len(indices_per_client[j]) > min_size]\n            for d in donors:\n                if len(indices_per_client[k]) >= min_size:\n                    break\n                # move one sample from donor d\n                indices_per_client[k].append(indices_per_client[d].pop())\n\n    return indices_per_client\n\n\ndef build_cifar_clients(data_root, num_clients=10, alpha=0.1, batch_size=32, test_batch_size=256, num_workers=2, seed=0):\n    \"\"\"\n    Loads CIFAR-10 and partitions training data among num_clients with Dirichlet(alpha).\n    Returns:\n      - clients: list of FedClient(name, train_loader, test_loader, device) instances (train loader per client)\n      - global_test_loader: DataLoader on full CIFAR-10 test set\n      - client_indices_list: list of lists of indices for reproducibility\n    \"\"\"\n    set_global_seed(seed)\n    # Download or use local copy\n    cifar_train = CIFAR10(root=data_root, train=True, download=True, transform=train_transform_cifar)\n    cifar_test = CIFAR10(root=data_root, train=False, download=True, transform=test_transform_cifar)\n    \n    # Get labels\n    train_targets = [int(x) for x in cifar_train.targets]\n    # partition\n    indices_per_client = dirichlet_partition_noniid(train_targets, num_clients, alpha, rng=np.random.RandomState(seed))\n    \n    # Build DataLoaders per client\n    clients = []\n    for k in range(num_clients):\n        idxs = indices_per_client[k]\n        subset = Subset(cifar_train, idxs)\n        loader = DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n        # name them client_0..client_{K-1}\n        clients.append((f\"client_{k}\", loader))\n    \n    # global test loader\n    test_loader = DataLoader(cifar_test, batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n    \n    return clients, test_loader, indices_per_client\n\n# Example usage (small example)\ndata_root = '/kaggle/working/cifar'\nclients, test_loader, idxs = build_cifar_clients(data_root, num_clients=3, alpha=0.1, batch_size=32)","metadata":{"id":"AEOJEMNuC9PE","outputId":"1e9f9e96-d3f0-4f90-ffc2-d3dd84fbe2bb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- helper to select \"head\" params (works for your SqueezeNet variants) ----\ndef get_head_param_list_and_names(model):\n    \"\"\"\n    Return (param_list, param_names) for the classifier/head parameters only.\n    Works for:\n      - model.fc (SqueezeNetLinear)\n      - model.backbone.classifier[1] (SqueezeNetClassifier backbone conv)\n      - last Linear layer named 'fc' anywhere else\n    \"\"\"\n    # SqueezeNetLinear: model.fc\n    if hasattr(model, \"fc\") and isinstance(model.fc, torch.nn.Linear):\n        params = [p for p in model.fc.parameters() if p.requires_grad]\n        names = [\"fc.\" + n for n, _ in model.fc.named_parameters()]\n        return params, names\n\n    # SqueezeNetClassifier with backbone.classifier[1] conv\n    if hasattr(model, \"backbone\") and hasattr(model.backbone, \"classifier\"):\n        conv = model.backbone.classifier[1]\n        params = [p for p in conv.parameters() if p.requires_grad]\n        names = []\n        # get full names relative to model\n        for n, p in model.named_parameters():\n            if n.startswith(\"backbone.classifier.1\"):\n                names.append(n)\n        return params, names\n\n    # fallback: last N parameters\n    all_params = [p for p in model.parameters() if p.requires_grad]\n    names = [n for n, _ in model.named_parameters() if _ is not None]\n    if len(all_params) == 0:\n        return [], []\n    # choose last parameter group\n    last = all_params[-1]\n    return [last], [names[-1]]\n\n# Uniform delta sampler (paper's U(-rho, rho) per-parameter, scaled by model norm)\ndef make_uniform_delta_from_seed(seed, prototype_vector, rho, device='cpu', scale_by_norm=False):\n    # use a local generator to avoid global torch RNG side-effects\n    gen = torch.Generator(device=device)\n    gen.manual_seed(int(seed) & 0xffffffff)\n    flat = prototype_vector.to(device)\n    uni = (torch.rand(flat.shape, generator=gen, device=device) * 2.0 - 1.0) * rho\n    if scale_by_norm:\n        model_norm = torch.norm(flat) + 1e-12\n        delta = uni * model_norm\n    else:\n        delta = uni\n    return delta","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def client_compute_scores_for_fedgga(model, loss_fn, data_loader, seeds, rho, device='cpu', \n                                     scale_by_norm=False, search_head_only=True):\n    \"\"\"\n    Compute reference gradient on one small batch, then for each seed:\n      - apply delta IN-PLACE to model parameters (fast)\n      - forward/backward to get g_k (torch tensor on device)\n      - compute cosine similarity on device between g_k and ref_grad\n      - revert the delta IN-PLACE\n    Returns:\n      scores (list of float) length == len(seeds),\n      ref_grad_numpy (np.array),\n      loss_ref (float),\n      losses_k (list of float) length == len(seeds)\n    \"\"\"\n    local_model = deepcopy(model).to(device)\n    local_model.train()\n\n    # Get one small batch for scoring and move to device\n    it = iter(data_loader)\n    xs, ys = next(it)\n    xs, ys = xs.to(device), ys.to(device)\n\n    # Optionally micro-batch if you want cheaper scoring\n    # micro_b = min(8, xs.shape[0])\n    # xs, ys = xs[:micro_b], ys[:micro_b]\n\n    # Select parameters that will be perturbed / measured\n    # param_list = [p for p in local_model.parameters() if p.requires_grad]\n    if search_head_only:\n        param_list, param_names = get_head_param_list_and_names(local_model)\n    else:\n        param_list = [p for p in local_model.parameters() if p.requires_grad]\n    if len(param_list) == 0:\n        return [0.0] * len(seeds), np.zeros(1), 0.0, [0.0] * len(seeds)\n        \n    numels = [p.numel() for p in param_list]\n    flat_theta = parameters_to_vector(param_list).detach().to(device)\n\n    local_model.zero_grad()\n    out = local_model(xs)\n    loss_ref_tensor = loss_fn(out, ys)\n    loss_ref = float(loss_ref_tensor.detach().cpu().item())\n    loss_ref_tensor.backward()\n\n    ref_grad_parts = []\n    for p in param_list:\n        g = p.grad\n        if g is None:\n            ref_grad_parts.append(torch.zeros(p.numel(), device=device))\n        else:\n            ref_grad_parts.append(g.detach().view(-1))\n    ref_grad_t = torch.cat(ref_grad_parts)            # on device\n    ref_grad_numpy = ref_grad_t.detach().cpu().numpy()\n\n    scores = []\n    losses_k = []\n\n    # Helper: apply delta in-place and revert\n    def apply_delta_inplace(delta_flat):\n        offset = 0\n        for p, n in zip(param_list, numels):\n            seg = delta_flat[offset: offset + n].view_as(p.data)\n            p.data.add_(seg)\n            offset += n\n\n    def revert_delta_inplace(delta_flat):\n        offset = 0\n        for p, n in zip(param_list, numels):\n            seg = delta_flat[offset: offset + n].view_as(p.data)\n            p.data.sub_(seg)\n            offset += n\n\n    # Loop over all candidate seeds -> produce one score per seed\n    for seed in seeds:\n        delta = make_uniform_delta_from_seed(seed, flat_theta, rho, device=device, scale_by_norm=scale_by_norm)\n        apply_delta_inplace(delta)\n\n        local_model.zero_grad()\n        out_k = local_model(xs)\n        loss_k_tensor = loss_fn(out_k, ys)\n        loss_k = float(loss_k_tensor.detach().cpu().item())\n        loss_k_tensor.backward()\n\n        # collect gk as a single torch tensor (on device)\n        gk_parts = []\n        for p in param_list:\n            g = p.grad\n            if g is None:\n                gk_parts.append(torch.zeros(p.numel(), device=device))\n            else:\n                gk_parts.append(g.detach().view(-1))\n        gk_t = torch.cat(gk_parts)\n\n        # cosine sim on device (use small eps)\n        denom = (torch.norm(gk_t) * torch.norm(ref_grad_t) + 1e-12)\n        sim_t = float(torch.dot(gk_t, ref_grad_t).item() / denom.item())\n\n        scores.append(sim_t)\n        losses_k.append(loss_k)\n\n        # revert delta in-place\n        revert_delta_inplace(delta)\n\n    # Safety: ensure local_model params exactly restored (optional)\n    set_flat_params_to_model(local_model, flat_theta)\n\n    return scores, ref_grad_numpy, loss_ref, losses_k\n\ndef fedavg_from_state_dicts(state_dicts):\n    \"\"\"\n    Given a list of PyTorch state_dict() objects (assumed identical keys),\n    return a new state_dict that is the simple average of the tensors.\n    \"\"\"\n    if len(state_dicts) == 0:\n        raise ValueError(\"No state dicts provided to fedavg_from_state_dicts\")\n    n = len(state_dicts)\n    keys = list(state_dicts[0].keys())\n    new_sd = {}\n    for k in keys:\n        # sum up as float32 to avoid dtype issues\n        accum = None\n        for sd in state_dicts:\n            v = sd[k].cpu().float()\n            if accum is None:\n                accum = v.clone()\n            else:\n                accum += v\n        new_sd[k] = (accum / float(n))\n    return new_sd\n","metadata":{"id":"BHhTMDBLIUkw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import mode\n\nclass FedClient:\n    def __init__(self, name, train_loader, test_loader, device):\n        self.name = name\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n        self.device = device\n\n    def score_seeds(self, model, loss_fn, seeds, rho, scale_by_norm, search_head_only):\n        return client_compute_scores_for_fedgga(model, loss_fn, self.train_loader, seeds, rho, device=self.device, \n                                                scale_by_norm=scale_by_norm, search_head_only=search_head_only)\n\n    def local_update(self, global_model, local_epochs=1, lr=0.01, max_steps=None, use_amp=False):\n        model = deepcopy(global_model).to(self.device)\n        # opt = torch.optim.Adam(model.parameters(), lr=lr)\n        trainable_params = [p for p in model.parameters() if p.requires_grad]\n        if len(trainable_params) == 0:\n            return deepcopy(model.state_dict())\n            \n        opt = torch.optim.Adam(trainable_params, lr=lr)\n            \n        loss_fn = nn.CrossEntropyLoss()\n        model.train()\n\n        scaler = GradScaler() if (use_amp and self.device.startswith('cuda')) else None\n\n        step = 0\n        for _ in range(local_epochs):\n            for xb, yb in self.train_loader:\n                xb, yb = xb.to( self.device), yb.to( self.device)\n                opt.zero_grad()\n                if scaler is not None:\n                    with autocast():\n                        logits = model(xb)\n                        loss = loss_fn(logits, yb)\n                    scaler.scale(loss).backward()\n                    scaler.step(opt)\n                    scaler.update()\n                else:\n                    logits = model(xb)\n                    loss = loss_fn(logits, yb)\n                    loss.backward()\n                    opt.step()\n                step += 1\n                if (max_steps is not None) and (step >= max_steps):\n                    break\n            if (max_steps is not None) and (step >= max_steps):\n                break\n\n        return deepcopy(model.state_dict())\n    \n    def compute_avg_gradient(self, global_model, local_epochs=1, max_batches=None, device=None):\n        \"\"\"\n        Compute average gradients over local data WITHOUT applying optimizer steps.\n        Use a deepcopy of `global_model` (server model) so we don't require client-local model attr.\n        Returns: dict mapping parameter name -> gradient tensor (on CPU).\n        \"\"\"\n        device = device or self.device\n        # local copy of server/global model to avoid modifying server state\n        model = deepcopy(global_model).to(device)\n        model.train()\n\n        loss_fn = nn.CrossEntropyLoss()\n\n        accumulated = None\n        batch_count = 0\n        step = 0\n\n        for ep in range(local_epochs):\n            for xb, yb in self.train_loader:\n                if (max_batches is not None) and (step >= max_batches):\n                    break\n                xb, yb = xb.to(device), yb.to(device)\n                model.zero_grad()\n                out = model(xb)\n                loss = loss_fn(out, yb)\n                loss.backward()\n\n                # collect this batch grads (move to CPU to keep memory predictable)\n                batch_grads = {}\n                for name, param in model.named_parameters():\n                    if param.grad is not None:\n                        batch_grads[name] = param.grad.detach().cpu().clone()\n\n                if accumulated is None:\n                    accumulated = {k: v.clone() for k, v in batch_grads.items()}\n                else:\n                    for k, v in batch_grads.items():\n                        if k in accumulated:\n                            accumulated[k] += v\n                        else:\n                            accumulated[k] = v.clone()\n\n                batch_count += 1\n                step += 1\n            if (max_batches is not None) and (step >= max_batches):\n                break\n\n        if accumulated is None:\n            return {}\n\n        for k in accumulated:\n            accumulated[k] = accumulated[k] / float(batch_count)\n\n        return accumulated  \n\n\n    def eval_on_test(self, model):\n        model = deepcopy(model).to(self.device)\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for x,y in self.test_loader:\n                x,y = x.to(self.device), y.to(self.device)\n                preds = model(x).argmax(dim=1)\n                correct += (preds == y).sum().item()\n                total += y.size(0)\n        return correct / total if total > 0 else 0.0\n\nclass FedGGAServer:\n    def __init__(self, server_model, clients, device, config, test_loader):\n        self.model = deepcopy(server_model).to(device)\n        self.clients = clients\n        self.device = device\n        self.config = config.copy()\n        self.log = []\n        self.W_history = []\n        self.test_loader = test_loader\n\n\n    def run(self):\n        cfg = self.config\n        loss_fn = nn.CrossEntropyLoss()\n        extra_grad_evals = 0\n        for rnd in range(cfg['rounds']):\n            client_state_dicts = []\n            client_flat_updates = []\n            applied = False\n            \n            t0 = time.time()\n            \n            if cfg['R_start'] <= rnd <= cfg['R_end'] and cfg['enable_gga']:\n                seeds = sample_k_seeds(cfg['K'], base_seed=cfg.get('base_seed', 1234) + rnd)\n                # start_seed = cfg.get('base_seed', 1234)\n                # current_round_seed = start_seed * 1000 + (rnd * cfg['K']) \n                # seeds = sample_k_seeds(cfg['K'], base_seed=current_round_seed)\n                \n                client_scores = []   # shape (n_clients, K)\n                client_ref_grads = []\n                client_ref_losses = []\n                client_losses_k = [] # list of lists: for each client, list of K losses\n\n                # print(\"clients starting\")\n                # Each client computes scores and returns ref_grad & losses\n                for c in self.clients:\n                    scores, ref_grad, ref_loss, losses_k = c.score_seeds(self.model, loss_fn, seeds, cfg['rho'], \n                                                             scale_by_norm=cfg.get('scale_by_norm', False),\n                                                             search_head_only=cfg.get('search_last_layer_only', True))\n                    client_scores.append(scores)\n                    client_ref_grads.append(ref_grad)\n                    client_ref_losses.append(ref_loss)\n                    client_losses_k.append(losses_k)\n                    extra_grad_evals += (1 + len(seeds)) * 1  # approx: 1 ref + K candidates per client\n\n                # compute reference sim LB: min pairwise among client_ref_grads\n                sim_ref_min, sim_ref_mean = pairwise_cosine_stats(client_ref_grads)\n                LB = float(np.mean(client_ref_losses))  # average ref loss across clients\n\n                # aggregate candidate scores/losses across clients\n                arr_scores = np.stack(client_scores, axis=0)  # (n_clients, K)\n                avg_scores = np.mean(arr_scores, axis=0)     # average of per-client sim proxies\n\n                # --- START OF DIAGNOSTIC CHECK ---\n                # agg_min = np.min(avg_scores)\n                # agg_max = np.max(avg_scores)\n                # agg_mean = np.mean(avg_scores)\n                \n                # print(f\"[Diag Round {rnd}] Agreement (avg_scores): min={agg_min:.4e}, max={agg_max:.4e}, mean={agg_mean:.4e}\")\n                \n                # if agg_mean < 0.1:\n                #     print(f\"DEBUG ALERT: Agreement weights are tiny (mean={agg_mean:.4f} << 0.1).\")\n                #     print(\"Suggestions: Relax 'loss_relax', use larger batches, or check if 'rho' is too large/small.\")\n                # --- END OF DIAGNOSTIC CHECK ---\n                \n                arr_losses_k = np.stack(client_losses_k, axis=0)  # (n_clients, K)\n                avg_losses_k = np.mean(arr_losses_k, axis=0)\n\n                # apply acceptance rule per candidate: avg_scores[k] > sim_ref_min AND avg_losses_k[k] - LB < loss_relax\n                loss_relax = cfg.get('loss_relax', 0.1)\n                accepted_indices = []\n                for k_idx in range(len(seeds)):\n                    if (avg_scores[k_idx] > sim_ref_min) and ((avg_losses_k[k_idx] - LB) < loss_relax):\n                        accepted_indices.append(k_idx)\n\n                # choose best among accepted by highest avg_score (if none accepted, choose best avg_score but only if not too much loss)\n                if len(accepted_indices) > 0:\n                    best_k = int(np.argmax(avg_scores[accepted_indices]))\n                    best_k = accepted_indices[best_k]\n                else:\n                    # fallback: choose argmax avg_scores but require loss condition (if not satisfied, skip applying any delta)\n                    best_k = int(np.argmax(avg_scores))\n                    if not ((avg_losses_k[best_k] - LB) < loss_relax):\n                        best_k = None\n\n                # apply delta if best_k exists\n                if best_k is not None:\n                    best_seed = seeds[best_k]\n                    if cfg.get('search_last_layer_only', True):\n                        server_param_list, server_param_names = get_head_param_list_and_names(self.model)\n                    else:\n                        server_param_list = [p for p in self.model.parameters() if p.requires_grad]\n                    \n                    flat_theta = parameters_to_vector(server_param_list).detach().to(self.device)\n                    delta = make_uniform_delta_from_seed(best_seed, flat_theta.cpu(), cfg['rho'], device=self.device,\n                                                        scale_by_norm=cfg.get('scale_by_norm'))\n                    beta = cfg.get('beta', 1.0)\n                    new_flat = (flat_theta.to(self.device) + beta * delta.to(self.device)).clone()\n                    \n                    offset = 0\n                    for p, n in zip(server_param_list, [p.numel() for p in server_param_list]):\n                        seg = new_flat[offset: offset + n].view_as(p.data)\n                        p.data.copy_(seg)\n                        offset += n\n                    # set_flat_params_to_model(self.model, new_flat)\n                    applied = True\n                else:\n                    applied = False\n\n                # diagnostics\n                min_sim = sim_ref_min\n                mean_sim = sim_ref_mean\n\n\n            elif cfg['enable_dampening'] and (rnd >= cfg.get(\"D_start\") and rnd < cfg.get(\"P_start\")):\n                aggregation = AgreementWeightedFedAvg(verbose=cfg.get('agg_verbose', False))\n            \n                # Use server model param order (only trainable params) to ensure consistent flattening\n                server_param_names = [name for name, p in self.model.named_parameters() if p.requires_grad]\n                param_numels = {name: int(p.numel()) for name, p in self.model.named_parameters() if p.requires_grad}\n            \n                # 1) compute avg gradients per client (CPU tensors) using the current server model snapshot\n                gradient_dicts = []\n                for c in self.clients:\n                    grads_cpu = c.compute_avg_gradient(\n                        global_model=self.model,\n                        local_epochs=cfg['local_epochs'],\n                        max_batches=cfg['max_client_steps'],\n                        device=self.device\n                    )\n                    if not grads_cpu:\n                        # client didn't return gradients (e.g. empty loader) -> skip\n                        continue\n            \n                    # normalize/ensure grads_cpu has CPU torch.Tensor values for server_param_names\n                    # fill missing params with zeros of correct size\n                    grads_fixed = {}\n                    for name in server_param_names:\n                        if name in grads_cpu:\n                            # ensure it's a torch.Tensor on CPU\n                            t = grads_cpu[name]\n                            if isinstance(t, np.ndarray):\n                                t = torch.from_numpy(t)\n                            grads_fixed[name] = t.detach().cpu().clone()\n                        else:\n                            grads_fixed[name] = torch.zeros(param_numels[name], dtype=torch.float32)\n            \n                    # minimal wrapper object expected by AgreementWeightedFedAvg (has .gradients and .client_id)\n                    class _G:\n                        def __init__(self, d, cid):\n                            self.gradients = d\n                            self.client_id = cid\n                    gradient_dicts.append(_G(grads_fixed, c.name))\n            \n                # If no gradients, fallback to normal FedAvg local updates (keeps loop central)\n                if len(gradient_dicts) == 0:\n                    client_state_dicts = []\n                    for c in self.clients:\n                        sd = c.local_update(self.model, local_epochs=cfg['local_epochs'],\n                                            lr=cfg['local_lr'], max_steps=cfg['max_client_steps'], use_amp=cfg['use_amp'])\n                        client_state_dicts.append(sd)\n                    new_sd = fedavg_from_state_dicts(client_state_dicts)\n                    self.model.load_state_dict(new_sd)\n            \n                    pct_pruned = 0.0\n                    damp_W_mean = None\n                    min_sim, mean_sim = None, None\n                else:\n                    # 2) compute flattened numpy vectors (CPU) for pairwise similarity stats\n                    flat_list = []\n                    for g in gradient_dicts:\n                        parts = []\n                        for name in server_param_names:\n                            t = g.gradients.get(name)\n                            if t is None:\n                                parts.append(np.zeros(param_numels[name], dtype=np.float32))\n                            else:\n                                parts.append(t.reshape(-1).numpy())\n                        flat_vec = np.concatenate(parts).astype(np.float32)\n                        flat_list.append(flat_vec)\n            \n                    # compute pairwise stats (min and mean) using your pairwise_cosine_stats utility\n                    min_sim, mean_sim = pairwise_cosine_stats(flat_list)  # expects list of 1D numpy arrays\n            \n                    # 3) aggregate with AgreementWeightedFedAvg (works on CPU tensors)\n                    weighted_grad = aggregation.aggregate(gradient_dicts)   # dict: param_name -> CPU tensor\n            \n                    # 4) apply aggregated gradient to server model using server_lr_damp (single server update)\n                    server_lr_damp = cfg.get('server_lr') * 10\n                    with torch.no_grad():\n                        for name, param in self.model.named_parameters():\n                            if name in weighted_grad:\n                                g_cpu = weighted_grad[name]  # CPU tensor\n                                # if g_cpu is numpy, convert\n                                if isinstance(g_cpu, np.ndarray):\n                                    g_cpu = torch.from_numpy(g_cpu)\n                                param.data.add_(-server_lr_damp * g_cpu.to(param.device))\n            \n                    # diagnostics: agreement stats (optional)\n                    try:\n                        damp_W_mean = float(np.mean([float(v.mean()) for v in aggregation.last_agreement_weights.values()])) \\\n                                     if hasattr(aggregation, 'last_agreement_weights') and aggregation.last_agreement_weights else None\n                    except Exception:\n                        damp_W_mean = None\n            \n                    pct_pruned = 0.0\n            \n            elif cfg['enable_pruning'] and (rnd >= cfg.get(\"P_start\") and rnd <= cfg.get(\"rounds\")):\n                aggregation = PruningFedAvg(threshold=cfg.get('P_tolerance'), patience=cfg.get('P_patience'))\n            \n                # 1) collect avg gradients (CPU tensors) from each client using same centralized loop\n                gradient_dicts = []\n                param_order = None\n                for c in self.clients:\n                    grads_cpu = c.compute_avg_gradient(\n                        global_model=self.model,\n                        local_epochs=cfg.get('local_epochs', 1),\n                        max_batches=cfg.get('max_client_steps', None),\n                        device=self.device\n                    )\n                    if not grads_cpu:\n                        continue\n                    if param_order is None:\n                        param_order = list(grads_cpu.keys())\n                    class _G:\n                        def __init__(self, d, cid):\n                            self.gradients = d\n                            self.client_id = cid\n                    gradient_dicts.append(_G(grads_cpu, c.name))\n            \n                # If no gradients were collected, fallback to normal FedAvg round so loop remains centralized\n                if len(gradient_dicts) == 0:\n                    client_state_dicts = []\n                    for c in self.clients:\n                        sd = c.local_update(self.model, local_epochs=cfg['local_epochs'],\n                                            lr=cfg['local_lr'], max_steps=cfg['max_client_steps'], use_amp=cfg['use_amp'])\n                        client_state_dicts.append(sd)\n                    new_sd = fedavg_from_state_dicts(client_state_dicts)\n                    self.model.load_state_dict(new_sd)\n                    pct_pruned = 0.0\n                    damp_W_mean = None\n                    min_sim, mean_sim = None, None\n                else:\n                    # 2) Flatten per-client average gradients (numpy) to compute pairwise similarity diagnostics\n                    flat_list = []\n                    for g in gradient_dicts:\n                        parts = []\n                        for name in param_order:\n                            arr = g.gradients.get(name)\n                            if arr is None:\n                                parts.append(np.zeros(0, dtype=float))   # unlikely, fallback\n                            else:\n                                parts.append(arr.reshape(-1).numpy())\n                        flat_list.append(np.concatenate(parts))\n                    # compute pairwise stats (returns min_sim, mean_sim over clients)\n                    min_sim, mean_sim = pairwise_cosine_stats(flat_list)\n            \n                    # 3) Aggregate with PruningFedAvg -> it returns (pruned_grads, pruning_rate)\n                    pruned_grads, pruning_rate = aggregation.aggregate(gradient_dicts)\n            \n                    # 4) Apply pruned gradients to server model in-place using server_lr_prune\n                    server_lr_prune = cfg.get('server_lr', 1e-3) * 10\n                    with torch.no_grad():\n                        for name, param in self.model.named_parameters():\n                            if name in pruned_grads:\n                                g_cpu = pruned_grads[name]   # CPU tensor\n                                param.data.add_(-server_lr_prune * g_cpu.to(self.device))\n            \n                    # diagnostics\n                    pct_pruned = pruning_rate\n                    try:\n                        agree_stats = aggregation.get_agreement_statistics()\n                        damp_W_mean = np.mean([s['mean'] for s in agree_stats.values()]) if len(agree_stats) > 0 else None\n                    except Exception:\n                        damp_W_mean = None\n            \n            \n            \n            else:\n                flat_server = parameters_to_vector([p for p in self.model.parameters() if p.requires_grad]).detach().cpu().numpy()\n                \n                for c in self.clients:\n                    sd = c.local_update(self.model, cfg['local_epochs'], cfg['local_lr'], cfg['max_client_steps'], cfg['use_amp'])\n                    client_state_dicts.append(sd)\n                    \n                    # Compute update delta\n                    client_model = deepcopy(self.model)\n                    client_model.load_state_dict(sd)\n                    flat_client = parameters_to_vector([p for p in client_model.parameters() if p.requires_grad]).detach().cpu().numpy()\n                    client_flat_updates.append(flat_client - flat_server)\n                \n                min_sim, mean_sim = pairwise_cosine_stats(client_flat_updates)\n                self.model.load_state_dict(fedavg_from_state_dicts(client_state_dicts))\n                pct_pruned, damp_W_mean = 0.0, None\n\n            # evaluate (clients' local test loaders might be used; for held-out evaluation evaluate separately)\n            accs = [c.eval_on_test(self.model) for c in self.clients]\n            avg_acc = float(np.mean(accs))\n\n            # logging\n            self.log.append({\n                'round': rnd,\n                'avg_client_acc': avg_acc,\n                'min_pairwise_sim': min_sim,\n                'mean_pairwise_sim': mean_sim,\n                'applied_delta': bool(applied),\n                'applied_delta': applied,\n                'pct_pruned': pct_pruned,\n                'damp_W_mean': damp_W_mean,\n                'time': time.time() - t0,\n                'extra_grad_evals_est': extra_grad_evals\n            })\n\n            if rnd%5==0:\n              print(f\"[R{rnd}] avg_acc={avg_acc:.4f} min_sim={min_sim} mean_sim={mean_sim} applied_delta={applied}\")\n              # print(f\"[R{rnd}] avg_acc={avg_acc:.4f} min_sim={min_sim} mean_sim={mean_sim} applied_delta={applied} pct_pruned={pct_pruned:.3f}\")\n\n        return self.log","metadata":{"id":"oTFnv8S4A4To","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SmallCNN(nn.Module):\n  def __init__(self, num_classes=10):\n      super().__init__()\n      # input 3x224x224 (same preprocessing). You can shrink the input if desired.\n      self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   # out: 32x224x224\n      self.bn1   = nn.BatchNorm2d(32)\n      self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # out: 64x224x224\n      self.bn2   = nn.BatchNorm2d(64)\n      self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # out: 128x224x224\n      self.bn3   = nn.BatchNorm2d(128)\n      self.pool  = nn.MaxPool2d(2)  # halves spatial dims\n      self.avgpool = nn.AdaptiveAvgPool2d((1,1))  # global pooling\n      self.fc = nn.Linear(128, num_classes)\n\n  def forward(self, x):\n      x = F.relu(self.bn1(self.conv1(x)))\n      x = self.pool(x)                    # 32 x 112 x 112\n      x = F.relu(self.bn2(self.conv2(x)))\n      x = self.pool(x)                    # 64 x 56 x 56\n      x = F.relu(self.bn3(self.conv3(x)))\n      x = self.pool(x)                    # 128 x 28 x 28\n      x = self.avgpool(x)                 # 128 x 1 x 1\n      x = x.view(x.size(0), -1)           # 128\n      x = self.fc(x)                      # num_classes\n      return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport pandas as pd\nfrom copy import deepcopy\n\ndef eval_global_on_test(model, test_loader, device):\n    model = deepcopy(model).to(device)\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for xb, yb in test_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = model(xb).argmax(dim=1)\n            correct += (preds == yb).sum().item()\n            total += yb.size(0)\n    return correct / total if total > 0 else 0.0\n\ndef run_fed_cifar_experiments(data_root,\n                              num_clients=10,\n                              seeds=None,\n                              alpha=0.1,\n                              optimizer='adam',\n                              save_dir='/kaggle/working/fed_gga_cifar',\n                              client_batch_size=32,\n                              test_batch_size=256,\n                              use_pretrained=True,\n                              freeze_backbone=False):\n    \n    os.makedirs(save_dir, exist_ok=True)\n\n    cfg = {}\n    cfg.setdefault('rounds', 50)\n    cfg.setdefault('R_start', 2)\n    cfg.setdefault('R_end', 15)\n    cfg.setdefault('D_start', 20)\n    cfg.setdefault('P_start', 50)\n    cfg.setdefault('P_tolerance', 0.2)\n    cfg.setdefault('P_patience', 1)\n    cfg.setdefault('K', 8)\n    cfg.setdefault('rho', 1e-5)\n    cfg.setdefault('beta', 0.3)\n    cfg.setdefault('local_epochs', 2)\n    cfg.setdefault('max_client_steps', 100)\n    cfg.setdefault('local_lr',  1e-3)\n    cfg.setdefault('server_lr', 1e-3)\n    cfg.setdefault('enable_gga', True)\n    cfg.setdefault('enable_dampening', True)\n    cfg.setdefault('enable_pruning', False)\n    cfg.setdefault('use_amp', True)\n    cfg.setdefault('scale_by_norm', True)\n    cfg.setdefault('search_last_layer_only', False)\n    cfg.setdefault('loss_relax', 0.05)\n\n    runs = []\n\n    set_global_seed(seed)\n    clients_list, global_test_loader, indices_per_client = build_cifar_clients(\n        data_root,\n        num_clients=num_clients,\n        alpha=alpha,\n        batch_size=client_batch_size,\n        test_batch_size=test_batch_size,\n        num_workers=2,\n        seed=seed\n    )\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    fed_clients = [FedClient(name, train_loader, global_test_loader, device) for name, train_loader in clients_list]\n    \n    model = SmallCNN().to(device)\n\n    server = FedGGAServer(model, fed_clients, device, cfg.copy(), global_test_loader)\n\n    t0 = time.time()\n    run_log = server.run()\n    run_time = time.time() - t0\n\n    global_acc = eval_global_on_test(server.model, global_test_loader, device)\n    print('evaluation done')\n\n    client_sizes = [len(idxs) for idxs in indices_per_client]\n\n    run_id = f\"cifar_alpha{alpha}_clients{num_clients}_seed{seed}\"\n    pd.DataFrame(run_log).to_csv(os.path.join(save_dir, f\"runlog_{run_id}.csv\"), index=False)\n\n    runs.append({\n        'dataset': 'CIFAR10',\n        'alpha': alpha,\n        'num_clients': num_clients,\n        'seed': seed,\n        'optimizer': optimizer,\n        'global_test_acc': float(global_acc),\n        'time_s': float(run_time),\n        'cfg_K': cfg['K'],\n        'cfg_rho': cfg['rho'],\n        'cfg_R_start': cfg['R_start'],\n        'cfg_R_end': cfg['R_end'],\n        'mean_client_size': float(np.mean(client_sizes)),\n    })\n\n    print(f\"[seed {seed}] global_test_acc={global_acc:.4f} time={run_time:.1f}s mean_client_size={np.mean(client_sizes):.1f}\")\n\n    df = pd.DataFrame(runs)\n    df.to_csv(os.path.join(save_dir, \"per_run_results_cifar.csv\"), index=False)\n\n    summary = df.groupby(['alpha','num_clients'])['global_test_acc'].agg(['mean','std']).reset_index()\n\n    table_rows = {}\n    for _, row in summary.iterrows():\n        k = f\"alpha={row['alpha']}_clients={int(row['num_clients'])}\"\n        table_rows[k] = f\"{float(row['mean']):.4f} ± {float(row['std']):.4f}\"\n\n    summary.to_csv(os.path.join(save_dir, \"per_config_summary_cifar.csv\"), index=False)\n    print(\"\\nSaved CIFAR run results to:\", save_dir)\n\n    return df, summary, table_rows, server.model","metadata":{"id":"MFl4XomSJY0k","outputId":"756b5811-d77f-445c-9db8-ccd4a845b947","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df, summary, table, trained_model = run_fed_cifar_experiments(\n    data_root='/kaggle/working/cifar',\n    seeds=[0],\n    num_clients=3,\n    alpha=0.1,\n    save_dir='/kaggle/working/fed_gga_cifar'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop-in code: reads saved CSV runlog, plots acc vs mean_pairwise_sim, returns metrics\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef load_runlog_csv(path):\n    \"\"\"Load runlog CSV and normalize round column.\"\"\"\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Runlog not found: {path}\")\n    df = pd.read_csv(path)\n    if 'round' in df.columns:\n        df['round'] = df['round'].astype(int)\n        df = df.sort_values('round').reset_index(drop=True)\n    return df\n\ndef select_accuracy_series(df, acc_priority=('global_accu','avg_client_acc')):\n    \"\"\"Return (acc_series (pd.Series indexed by round), acc_col_name).\"\"\"\n    acc_col = None\n    for c in acc_priority:\n        if c in df.columns:\n            acc_col = c\n            break\n    if acc_col is None:\n        raise ValueError(f\"No accuracy column found in runlog. Expected one of {acc_priority}\")\n    s = pd.to_numeric(df[acc_col], errors='coerce')\n    # fill edge NaNs then interior via forward/backward fill\n    s = s.fillna(method='ffill').fillna(method='bfill')\n    if 'round' in df.columns:\n        s.index = df['round'].values\n    else:\n        s.index = np.arange(len(s))\n    return s, acc_col\n\ndef select_similarity_series(df, sim_col='mean_pairwise_sim'):\n    \"\"\"Return sim_series (pd.Series indexed by round). If not present, returns NaN series.\"\"\"\n    if sim_col in df.columns:\n        s = pd.to_numeric(df[sim_col], errors='coerce')\n        s = s.fillna(method='ffill').fillna(method='bfill')\n        if 'round' in df.columns:\n            s.index = df['round'].values\n        else:\n            s.index = np.arange(len(s))\n    else:\n        # create NaN series of same length/index as accuracy\n        n = len(df)\n        idx = df['round'].values if 'round' in df.columns else np.arange(n)\n        s = pd.Series([np.nan]*n, index=idx)\n    return s\n\ndef compute_metrics_from_series(acc_s, sim_s):\n    \"\"\"Compute final_acc, mean_pairwise_sim, auc (normalized by round span).\"\"\"\n    if len(acc_s) == 0:\n        raise ValueError(\"Empty accuracy series.\")\n    final_acc = float(acc_s.iloc[-1])\n    # mean similarity ignoring NaNs\n    mean_sim = float(sim_s.dropna().mean()) if sim_s.dropna().size > 0 else float('nan')\n    # AUC (trapezoid) over rounds normalized by (last_round - first_round)\n    rounds = np.asarray(acc_s.index, dtype=float)\n    if len(rounds) >= 2:\n        auc_raw = np.trapz(y=acc_s.values, x=rounds)\n        denom = (rounds[-1] - rounds[0]) if (rounds[-1] - rounds[0]) > 0 else 1.0\n        auc = float(auc_raw / denom)\n    else:\n        auc = float(acc_s.iloc[-1])\n    return {'final_acc': final_acc, 'mean_pairwise_sim': mean_sim, 'auc': auc}\n\ndef plot_from_runlog(runlog_path, out_fig=None, title=None, show=True):\n    \"\"\"\n    Read CSV at runlog_path, plot accuracy (left y) and mean_pairwise_sim (right y).\n    Returns: metrics dict and (acc_series, sim_series, df) for further use.\n    \"\"\"\n    df = load_runlog_csv(runlog_path)\n    acc_s, acc_col = select_accuracy_series(df)\n    sim_s = select_similarity_series(df, sim_col='mean_pairwise_sim')\n\n    # metrics\n    metrics = compute_metrics_from_series(acc_s, sim_s)\n\n    # plotting\n    fig, ax1 = plt.subplots(figsize=(9,5))\n    rounds = acc_s.index.values\n\n    # accuracy (left)\n    ax1.plot(rounds, acc_s.values, marker='o', linewidth=2, label=f'Accuracy ({acc_col})')\n    ax1.set_xlabel('round')\n    ax1.set_ylabel('accuracy')\n    ax1.grid(True)\n\n    # similarity (right)\n    ax2 = ax1.twinx()\n    # if sim is all NaN, draw nothing and warn\n    if np.all(np.isnan(sim_s.values)):\n        ax2.text(0.5, 0.5, 'no similarity data', transform=ax2.transAxes, ha='center', va='center', alpha=0.7)\n        ax2.set_ylabel('mean_pairwise_sim (N/A)')\n    else:\n        ax2.plot(rounds, sim_s.values, marker='x', linewidth=2, color='tab:orange', label='mean_pairwise_sim')\n        ax2.set_ylabel('mean_pairwise_sim')\n\n    # combined legend\n    lines_1, labels_1 = ax1.get_legend_handles_labels()\n    print(lines_1)\n    print(labels_1)\n    lines_2, labels_2 = ax2.get_legend_handles_labels()\n    print(lines_2)\n    print(labels_2)\n    if lines_2:\n        ax1.legend(lines_1 + lines_2, ['Accuracy (global model)'] + labels_2, loc='best')\n    else:\n        ax1.legend(lines_1, labels_1, loc='best')\n\n    if title is None:\n        title = os.path.basename(runlog_path)\n    plt.title(title)\n    plt.tight_layout()\n\n    if out_fig is not None:\n        os.makedirs(os.path.dirname(out_fig), exist_ok=True)\n        plt.savefig(out_fig, dpi=200)\n        print(\"Saved figure:\", out_fig)\n\n    if show:\n        plt.show()\n    else:\n        plt.close(fig)\n\n    # print metrics\n    print(\"Metrics:\")\n    print(f\"  final_acc = {metrics['final_acc']:.4f}\")\n    if not np.isnan(metrics['mean_pairwise_sim']):\n        print(f\"  mean_pairwise_sim = {metrics['mean_pairwise_sim']:.6f}\")\n    else:\n        print(\"  mean_pairwise_sim = N/A\")\n    print(f\"  auc (norm.) = {metrics['auc']:.6f}\")\n\n    return metrics, (acc_s, sim_s, df)\n\n\nrunlog_path = \"/kaggle/input/comb1-exp/runlog_cifar_alpha0.1_clients3_seed0_comb1.csv\"\nmetrics, (acc_s, sim_s, df) = plot_from_runlog(runlog_path,\n                                              out_fig=\"/kaggle/working/fed_gga_cifar/plots/acc_vs_sim_seed0.png\",\n                                              show=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"/kaggle/working/fed_gga_cifar/final_server_model.pth\"\ntorch.save(trained_model.state_dict(), model_path)\nprint(\"Saved final server model to:\", model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}